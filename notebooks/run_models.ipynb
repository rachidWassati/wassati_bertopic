{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/tmp/ipykernel_5747/718148591.py:29: DtypeWarning: Columns (10,11,15,19,20,21,23,34,35,42,43,44,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\", dtype={'year': str})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "df = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\", dtype={'year': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_huggingface(model_name, task, problem_type=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This function loads a model and tokenizer from a given model name, then creates a pipeline to perform a specified task.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "        task (str): The type of task to perform with the pipeline.\n",
    "        problem_type (str): The type of problem to solve (\"multi_label_classification\" for multi-label tasks).\n",
    "        **kwargs: Additional arguments to pass to the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        pipeline: A pipeline configured to perform the specified task with the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=problem_type)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    classifier = pipeline(task, model=model, tokenizer=tokenizer, **kwargs)\n",
    "    return classifier\n",
    "\n",
    "def add_single_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function merges the DataFrame of single-label predictions with the original DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a dictionary containing a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Convert the predictions to a DataFrame\n",
    "    prediction_results = pd.DataFrame(predictions)\n",
    "    prediction_results.rename(columns={'label': predicted_column_name}, inplace=True)\n",
    "    # # Reset the indices of the DataFrames (if necessary)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # prediction_results.reset_index(drop=True, inplace=True)\n",
    "    # Merge the original DataFrame with the prediction results\n",
    "    df_predicted = pd.concat([predicted_df, prediction_results], axis=1)\n",
    "    return df_predicted\n",
    "\n",
    "def add_multi_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column with multi-label predictions to the DataFrame and also adds two more columns for \n",
    "    the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a list of dictionaries, where each dictionary \n",
    "                            contains a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores, as well as \n",
    "                      columns for the best label and its score.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Keep the original predictions as they are (a list of dictionaries) and add them to the DataFrame as a new column\n",
    "    predicted_df[predicted_column_name] = predictions\n",
    "    # Add columns for the best label and its score\n",
    "    predicted_df[f'best_{predicted_column_name}'] = predicted_df[predicted_column_name].apply(lambda x: max(x.keys(), key=lambda k: x[k]) if x else None)\n",
    "    predicted_df[f'best_{predicted_column_name}_score'] = predicted_df[predicted_column_name].apply(lambda x: x[max(x.keys(), key=lambda k: x[k])] if x else None)\n",
    "    return predicted_df\n",
    "\n",
    "def make_predictions_df(classifier, df, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function makes predictions on a DataFrame of documents using a given classifier. It adds the predictions to \n",
    "    the DataFrame as new columns. If the classifier is for single-label classification, it adds one column for the \n",
    "    predicted label and one for the score. If the classifier is for multi-label classification, it adds one column \n",
    "    with a dictionary of label-score pairs for each document, and two additional columns for the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        classifier (pipeline): The Hugging Face pipeline object for making predictions.\n",
    "        df (pd.DataFrame): The DataFrame containing the documents to make predictions on. It must have a 'processed_data' \n",
    "                           column with the preprocessed text of each document.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predictions.\n",
    "    \"\"\"\n",
    "    # Get the list of documents from the DataFrame\n",
    "    docs = df[\"processed_data\"].tolist()\n",
    "    # Get predictions\n",
    "    predictions = classifier(docs)\n",
    "    \n",
    "    # Check if predictions is a list of dictionaries (single-label case)\n",
    "    if isinstance(predictions, list) and isinstance(predictions[0], dict):\n",
    "        df_predicted = add_single_label_predictions(df, predictions, predicted_column_name)\n",
    "    \n",
    "    # Multi-label case\n",
    "    elif isinstance(predictions, list) and isinstance(predictions[0], list):\n",
    "        df_predicted = add_multi_label_predictions(df, predictions, predicted_column_name)\n",
    "\n",
    "    return df_predicted\n",
    "\n",
    "classifier = load_model_huggingface(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text-classification\", max_length=512, truncation=True)\n",
    "predicted_df = make_predictions_df(classifier, df, 'sentiment_label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wassati",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
