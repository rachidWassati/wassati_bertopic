{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "import os\n",
    "import copy\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.colors import ListedColormap\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from typing import List, Union\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8627/2415188386.py:2: DtypeWarning: Columns (6,7,11,15,16,17,19,30,31,38,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_labelled.csv\")\n",
      "/tmp/ipykernel_8627/2415188386.py:3: DtypeWarning: Columns (7,8,12,16,17,18,20,31,32,39,40,41,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_sentiment_labelled.csv\")\n",
      "/tmp/ipykernel_8627/2415188386.py:4: DtypeWarning: Columns (8,9,13,17,18,19,21,32,33,40,41,42,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df3 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_emotions_labelled.csv\")\n",
      "/tmp/ipykernel_8627/2415188386.py:22: DtypeWarning: Columns (9,10,14,18,19,20,22,33,34,41,42,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df4 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Read the excel review file\n",
    "df = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_labelled.csv\")\n",
    "df2 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_sentiment_labelled.csv\")\n",
    "df3 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_emotions_labelled.csv\")\n",
    "\n",
    "# Create a processed_data to be iso with the preprocessing output from the OOP \n",
    "df3[\"processed_data\"] = df3[\"allComment\"]\n",
    "# Convert the string values in the 'predicted_labels' column into lists\n",
    "df3['predicted_labels'] = df3['predicted_labels'].apply(lambda x: ast.literal_eval(x))\n",
    "# Convert the string values in the 'predicted_scores' column into dictionaries\n",
    "df3['predicted_scores'] = df3['predicted_scores'].apply(lambda x: ast.literal_eval(x))\n",
    "# Convert the string values in the 'predicted_scores' column into dictionaries\n",
    "df3['proba_dict'] = df3['proba_dict'].apply(lambda x: ast.literal_eval(x))\n",
    "# Fill empty values in Market Segment column\n",
    "df3['Market Segment'] = df3['Market Segment'].fillna('Unknown')\n",
    "# Convertir la colonne de date en un objet datetime\n",
    "df3['Response Date'] = pd.to_datetime(df3['Response Date'])\n",
    "# Extraire l'année et le mois et les stocker dans une nouvelle colonne\n",
    "df3['year_month'] = df3['Response Date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "\n",
    "df4 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyse de Sentiments simple : positif - négatif - neutre\n",
    "\n",
    "def load_model_huggingface(model_name, task, problem_type=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This function loads a model and tokenizer from a given model name, then creates a pipeline to perform a specified task.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "        task (str): The type of task to perform with the pipeline.\n",
    "        problem_type (str): The type of problem to solve (\"multi_label_classification\" for multi-label tasks).\n",
    "        **kwargs: Additional arguments to pass to the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        pipeline: A pipeline configured to perform the specified task with the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=problem_type)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    classifier = pipeline(task, model=model, tokenizer=tokenizer, **kwargs)\n",
    "    return classifier\n",
    "\n",
    "def add_single_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function merges the DataFrame of single-label predictions with the original DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a dictionary containing a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Convert the predictions to a DataFrame\n",
    "    prediction_results = pd.DataFrame(predictions)\n",
    "    prediction_results.rename(columns={'label': predicted_column_name}, inplace=True)\n",
    "    # # Reset the indices of the DataFrames (if necessary)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # prediction_results.reset_index(drop=True, inplace=True)\n",
    "    # Merge the original DataFrame with the prediction results\n",
    "    df_predicted = pd.concat([predicted_df, prediction_results], axis=1)\n",
    "    return df_predicted\n",
    "\n",
    "def add_multi_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column with multi-label predictions to the DataFrame and also adds two more columns for \n",
    "    the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a list of dictionaries, where each dictionary \n",
    "                            contains a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores, as well as \n",
    "                      columns for the best label and its score.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Keep the original predictions as they are (a list of dictionaries) and add them to the DataFrame as a new column\n",
    "    predicted_df[predicted_column_name] = predictions\n",
    "    # Add columns for the best label and its score\n",
    "    predicted_df[f'best_{predicted_column_name}'] = predicted_df[predicted_column_name].apply(lambda x: max(x.keys(), key=lambda k: x[k]) if x else None)\n",
    "    predicted_df[f'best_{predicted_column_name}_score'] = predicted_df[predicted_column_name].apply(lambda x: x[max(x.keys(), key=lambda k: x[k])] if x else None)\n",
    "    return predicted_df\n",
    "\n",
    "def make_predictions_df(classifier, df, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function makes predictions on a DataFrame of documents using a given classifier. It adds the predictions to \n",
    "    the DataFrame as new columns. If the classifier is for single-label classification, it adds one column for the \n",
    "    predicted label and one for the score. If the classifier is for multi-label classification, it adds one column \n",
    "    with a dictionary of label-score pairs for each document, and two additional columns for the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        classifier (pipeline): The Hugging Face pipeline object for making predictions.\n",
    "        df (pd.DataFrame): The DataFrame containing the documents to make predictions on. It must have a 'processed_data' \n",
    "                           column with the preprocessed text of each document.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predictions.\n",
    "    \"\"\"\n",
    "    # Get the list of documents from the DataFrame\n",
    "    docs = df[\"processed_data\"].tolist()\n",
    "    # Get predictions\n",
    "    predictions = classifier(docs)\n",
    "    \n",
    "    # Check if predictions is a list of dictionaries (single-label case)\n",
    "    if isinstance(predictions, list) and isinstance(predictions[0], dict):\n",
    "        df_predicted = add_single_label_predictions(df, predictions, predicted_column_name)\n",
    "    \n",
    "    # Multi-label case\n",
    "    elif isinstance(predictions, list) and isinstance(predictions[0], list):\n",
    "        df_predicted = add_multi_label_predictions(df, predictions, predicted_column_name)\n",
    "\n",
    "    return df_predicted\n",
    "\n",
    "classifier = load_model_huggingface(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text-classification\", max_length=512, truncation=True)\n",
    "predicted_df = make_predictions_df(classifier, df4, 'sentiment_label')\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "# classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, max_length=512, truncation=True)\n",
    "\n",
    "# predictions = []\n",
    "# for review in [[x] for x in df.allComment.tolist()]:\n",
    "      ## Effectuer une prédiction pour le document en utilisant le classificateur\n",
    "#     prediction = classifier(review)\n",
    "      ## Ajouter la prédiction à la liste des prédictions\n",
    "#     predictions.append(prediction)\n",
    "\n",
    "## Création d'un DataFrame à partir des résultats de prédiction\n",
    "# res = pd.DataFrame([item for sublist in predictions for item in sublist])\n",
    "# df_pred = pd.concat([df, res], axis=1)\n",
    "\n",
    "# The model created a column 'label' to store the prediction, BUT the df already had one column 'label', then :\n",
    "# # Rename the second 'label' column to 'sentiment_label'\n",
    "# cols = df_pred.columns.tolist()\n",
    "# cols[len(cols) - 1 - cols[::-1].index('label')] = 'sentiment_label'\n",
    "# df_pred.columns = cols\n",
    "\n",
    "# df_pred.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_sentiment_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyse de Sentiments approfondie : par 28 émotions\n",
    "\n",
    "# # en multilabel, avec GPU et par batch pour accélerer le traitement\n",
    "\n",
    "# # Check if a CUDA-enabled GPU is available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\", max_length=512)\n",
    "\n",
    "# # Move the model to the GPU\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Set the batch size\n",
    "# batch_size = 2\n",
    "\n",
    "# # Create a list of label names\n",
    "# label_emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise','neutral']\n",
    "\n",
    "# # Initialize lists to store the predicted labels and scores\n",
    "# predicted_labels = []\n",
    "# predicted_scores = []\n",
    "\n",
    "# df3 = df2\n",
    "# # Iterate over the rows of the DataFrame in batches\n",
    "# for i in range(0, len(df3), batch_size):\n",
    "#     batch = df3[i:i+batch_size]\n",
    "#     texts = batch['allComment'].tolist()\n",
    "#     inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "#     # Move the inputs to the GPU\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "#     outputs = model(**inputs)\n",
    "#     probs = outputs.logits.sigmoid().detach().cpu().numpy()\n",
    "    \n",
    "#     # Apply a threshold to the probabilities to get the predicted labels\n",
    "#     threshold = 0.5\n",
    "#     labels = [[label_emotions[i] for i, prob in enumerate(prob_row) if prob > threshold] for prob_row in probs]\n",
    "    \n",
    "#     # Store the predicted labels and scores\n",
    "#     predicted_labels.extend(labels)\n",
    "#     scores = [{label_emotions[i]: prob for i, prob in enumerate(prob_row)} for prob_row in probs]\n",
    "#     # predicted_scores.extend(probs.tolist())\n",
    "#     predicted_scores.extend(scores)\n",
    "\n",
    "# # Add the predicted labels and scores as new columns in the DataFrame\n",
    "# df3['predicted_labels'] = predicted_labels\n",
    "# df3['predicted_scores'] = predicted_scores\n",
    "\n",
    "# # Convert the string values in the 'predicted_labels' column into lists\n",
    "# df3['predicted_labels'] = df3['predicted_labels'].apply(lambda x: ast.literal_eval(x))\n",
    "# # Convert the string values in the 'predicted_scores' column into dictionaries\n",
    "# df3['predicted_scores'] = df3['predicted_scores'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# # df3.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_emotions_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict(input_dict, conversion_type='keys_to_values'):\n",
    "    \"\"\"\n",
    "    This function converts between a key-to-value dictionary and a value-to-keys dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): The input dictionary. If conversion_type is 'keys_to_values', this should be a dictionary where \n",
    "                           the keys are the original keys and the values are the corresponding values. If conversion_type is \n",
    "                           'values_to_keys', this should be a dictionary where the keys are the original values and the values \n",
    "                           are lists of keys for each value.\n",
    "        conversion_type (str): The type of conversion to perform. Can be either 'keys_to_values' or 'values_to_keys'.\n",
    "\n",
    "    Returns:\n",
    "        dict: The converted dictionary. If conversion_type is 'keys_to_values', this will be a dictionary where the keys \n",
    "              are the original values and the values are lists of keys for each value. If conversion_type is 'values_to_keys', \n",
    "              this will be a dictionary where the keys are the original keys and the values are the corresponding values.\n",
    "    \"\"\"\n",
    "    if conversion_type == 'keys_to_values':\n",
    "        output_dict = {}\n",
    "        for key, value in input_dict.items():\n",
    "            if value not in output_dict:\n",
    "                output_dict[value] = []\n",
    "            output_dict[value].append(key)\n",
    "    elif conversion_type == 'values_to_keys':\n",
    "        output_dict = {key: value for value, keys in input_dict.items() for key in keys}\n",
    "    else:\n",
    "        raise ValueError(\"Invalid conversion_type. Must be either 'keys_to_values' or 'values_to_keys'.\")\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def add_grouping_column(df, key_column, group_dict, group_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column to a DataFrame with the group name of each key.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        key_column (str): The name of the column in df that contains the keys.\n",
    "        group_dict (dict): A dictionary where the keys are the original keys and the values are the corresponding group names.\n",
    "        group_column_name (str): The name of the new column to be added to df for the groups.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an added column for the groups.\n",
    "    \"\"\"\n",
    "    df[group_column_name] = df[key_column].map(group_dict)\n",
    "    return df\n",
    "\n",
    "# transform the multilabel results from the emotions analysis in single label\n",
    "# Define the groups\n",
    "positive_emotions = ['admiration','approval','gratitude','caring','realization','joy','optimism','love','excitement','amusement','relief']\n",
    "negative_emotions = ['disappointment','disapproval','annoyance','confusion','nervousness','fear','sadness','remorse','disgust','embarrassment','anger']\n",
    "neutral_emotions = ['neutral','desire','surprise','curiosity']\n",
    "# Create a dictionary where the keys are the group names and the values are the lists of labels\n",
    "groups = {\"positive\": positive_emotions, \"negative\": negative_emotions, \"neutral\": neutral_emotions}\n",
    "# Convert the groups to a group_dict\n",
    "group_dict = convert_dict(groups, conversion_type='values_to_keys')\n",
    "\n",
    "# Now you can use group_dict with add_grouping_column\n",
    "df3 = add_grouping_column(df3, \"predicted_label\", group_dict, \"sentiment_from_emotion_label\")\n",
    "\n",
    "# # Create a new column 'single_emotion_label' that contains the label with the highest score\n",
    "# df3['single_emotion_label'] = df3['predicted_scores'].apply(lambda x: max(x, key=x.get))\n",
    "# # Create a new column 'sentiment' that contains the sentiment of the emotion in the 'single_emotion_label' column\n",
    "# df3['single_sentiment_from_emotion'] = df3['single_emotion_label'].apply(lambda x: 'positive' if x in positive_emotions else ('negative' if x in negative_emotions else ('neutral' if x in neutral_emotions else 'unknown')))\n",
    "\n",
    "# df3.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\")\n",
    "\n",
    "# Use the explode() method to transform each element of the list into a row\n",
    "# new_df3 = df3.explode('predicted_labels')\n",
    "# new_df3 = new_df3.rename(columns={'predicted_labels': 'emotion_label'})\n",
    "\n",
    "# Pour retourner au df initial avec une liste de label\n",
    "# Group the rows by the original index and aggregate the 'emotion_label' column into a list\n",
    "# original_df = new_df.groupby(new_df.index).agg({'emotion_label': list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### comparaison entre analyse de sentiment simple et les résultats du single label issus de l'analyse d'émotions\n",
    "\n",
    "# Calculate the total number of rows for each sentiment in the 'sentiment_label' column\n",
    "total_positive = len(df3[df3['sentiment_label'] == 'positive'])\n",
    "total_negative = len(df3[df3['sentiment_label'] == 'negative'])\n",
    "total_neutral = len(df3[df3['sentiment_label'] == 'neutral'])\n",
    "\n",
    "# Calculate the number of matches for each sentiment\n",
    "positive_matches = len(df3[(df3['sentiment_label'] == 'positive') & (df3['single_sentiment_from_emotion'] == 'positive')])\n",
    "negative_matches = len(df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_from_emotion'] == 'negative')])\n",
    "neutral_matches = len(df3[(df3['sentiment_label'] == 'neutral') & (df3['single_sentiment_from_emotion'] == 'neutral')])\n",
    "\n",
    "# Calculate the percentage of matches for each sentiment\n",
    "positive_match_percent = positive_matches / total_positive * 100 if total_positive > 0 else 0\n",
    "negative_match_percent = negative_matches / total_negative * 100 if total_negative > 0 else 0\n",
    "neutral_match_percent = neutral_matches / total_neutral * 100 if total_neutral > 0 else 0\n",
    "\n",
    "# Calculate the number of differences for each sentiment\n",
    "positive_differences = total_positive - positive_matches\n",
    "negative_differences = total_negative - negative_matches\n",
    "neutral_differences = total_neutral - neutral_matches\n",
    "\n",
    "# Calculate the percentage of differences for each sentiment\n",
    "positive_difference_percent = positive_differences / total_positive * 100 if total_positive > 0 else 0\n",
    "negative_difference_percent = negative_differences / total_negative * 100 if total_negative > 0 else 0\n",
    "neutral_difference_percent = neutral_differences / total_neutral * 100 if total_neutral > 0 else 0\n",
    "\n",
    "# Calculate the distribution of differences for each sentiment\n",
    "positive_to_negative = len(df3[(df3['sentiment_label'] == 'positive') & (df3['single_sentiment_from_emotion'] == 'negative')])\n",
    "positive_to_neutral = len(df3[(df3['sentiment_label'] == 'positive') & (df3['single_sentiment_from_emotion'] == 'neutral')])\n",
    "negative_to_positive = len(df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_from_emotion'] == 'positive')])\n",
    "negative_to_neutral = len(df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_from_emotion'] == 'neutral')])\n",
    "neutral_to_positive = len(df3[(df3['sentiment_label'] == 'neutral') & (df3['single_sentiment_from_emotion'] == 'positive')])\n",
    "neutral_to_negative = len(df3[(df3['sentiment_label'] == 'neutral') & (df3['single_sentiment_from_emotion'] == 'negative')])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Positive matches: {positive_match_percent:.2f}%\")\n",
    "print(f\"Negative matches: {negative_match_percent:.2f}%\")\n",
    "print(f\"Neutral matches: {neutral_match_percent:.2f}%\")\n",
    "print(f\"Positive differences: {positive_difference_percent:.2f}%\")\n",
    "print(f\"Negative differences: {negative_difference_percent:.2f}%\")\n",
    "print(f\"Neutral differences: {neutral_difference_percent:.2f}%\")\n",
    "print(f\"Positive to Negative: {positive_to_negative / total_positive * 100:.2f}%\" if total_positive > 0 else \"Positive to Negative: N/A\")\n",
    "print(f\"Positive to Neutral: {positive_to_neutral / total_positive * 100:.2f}%\" if total_positive > 0 else \"Positive to Neutral: N/A\")\n",
    "print(f\"Negative to Positive: {negative_to_positive / total_negative * 100:.2f}%\" if total_negative > 0 else \"Negative to Positive: N/A\")\n",
    "print(f\"Negative to Neutral: {negative_to_neutral / total_negative * 100:.2f}%\" if total_negative > 0 else \"Negative to Neutral: N/A\")\n",
    "print(f\"Neutral to Positive: {neutral_to_positive / total_neutral * 100:.2f}%\" if total_neutral > 0 else \"Neutral to Positive: N/A\")\n",
    "print(f\"Neutral to Negative: {neutral_to_negative / total_neutral * 100:.2f}%\" if total_neutral > 0 else \"Neutral to Negative: N/A\")\n",
    "\n",
    "# Filter the DataFrame to only include rows where the 'sentiment_label' is 'negative' and the 'single_sentiment_label' is 'neutral'\n",
    "negative_to_neutral = df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_from_emotion'] == 'neutral')]\n",
    "\n",
    "# Calculate the distribution of emotions in the 'single_emotion_label' column\n",
    "emotion_distribution = negative_to_neutral['single_emotion_label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "emotion_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### chargement du model bertopic\n",
    "\n",
    "def load_bertopic_model(filename):\n",
    "    \"\"\"\n",
    "    Load a BERTopic model and associated data from a file.\n",
    "    \n",
    "    :param filename: The name of the file to load the data from.\n",
    "    :return: A tuple containing the loaded BERTopic model, topics, probs, and docs variables.\n",
    "    \"\"\"\n",
    "    # Load the BERTopic model\n",
    "    topic_model = BERTopic.load(filename)\n",
    "    \n",
    "    # Load the topics, probs, and docs variables\n",
    "    with open(filename + '_data.pkl', 'rb') as f:\n",
    "        topics, probs, embeddings, docs = pickle.load(f)\n",
    "    \n",
    "    return topic_model, topics, probs, embeddings, docs\n",
    "\n",
    "topic_model, topics, probs, embeddings, docs = load_bertopic_model('../models/raw_keybert_bertopic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/vectorizers/_ctfidf.py:69: RuntimeWarning: divide by zero encountered in divide\n",
      "  idf = np.log((avg_nr_samples / df)+1)\n"
     ]
    }
   ],
   "source": [
    "### Création du modèle bertopic aggrégé pour topics finaux\n",
    "\n",
    "def create_merged_model(docs, bertopic_model, topics_to_merge_dict, label_names_dict):\n",
    "    \"\"\"\n",
    "    Create a new BERTopic model by merging topics from an existing model.\n",
    "\n",
    "    This function takes as input a list of documents `docs`, an existing BERTopic model `bertopic_model`, a dictionary `topics_to_merge_dict` specifying which topics to merge, and a dictionary `label_names_dict` specifying the labels for the merged topics.\n",
    "\n",
    "    The function creates a deep copy of the input BERTopic model and merges the specified topics using the `merge_topics` method. Then, it sets the topic labels for the merged model using the `set_topic_labels` method and the provided `label_names_dict`.\n",
    "\n",
    "    The resulting merged BERTopic model is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        bertopic_model (BERTopic): The input BERTopic model to be merged.\n",
    "        topics_to_merge_dict (dict): A dictionary specifying which topics to merge. The keys are the topic numbers to be merged, and the values are the topic numbers into which they should be merged.\n",
    "        label_names_dict (dict): A dictionary specifying the labels for the merged topics. The keys are the topic numbers, and the values are the corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        BERTopic: The resulting merged BERTopic model.\n",
    "    \"\"\"\n",
    "    topic_model_merged = copy.deepcopy(bertopic_model)\n",
    "    topic_model_merged.merge_topics(docs, topics_to_merge_dict)\n",
    "\n",
    "    # Create a dictionary to match the aggregated name to their corresponding topic number\n",
    "    mergedtopic_labels_dict = {i-1: item for i, item in enumerate(label_names_dict)}\n",
    "    # Set topic labels for the aggregated model\n",
    "    topic_model_merged.set_topic_labels(mergedtopic_labels_dict)\n",
    "\n",
    "    return topic_model_merged\n",
    "\n",
    "# List of topics numbers. Each value of this list is a list that contains the topic number of the topics to join together\n",
    "topics_to_merge = [ [42,3,0,13], #Delivery Deadlines : challenges and strategies involved in managing delivery deadlines in logistics operations. (vert)\n",
    "                    [20,50,27], #Quotation and Pricing Strategies (vert bas)\n",
    "                    [35,32], #Touch Panels and Screens (rouge, haut)\n",
    "                    [40,36], #Frequency Converters : frequency converters used in industrial applications and the technical support provided by manufacturers and suppliers (rouge, suite)\n",
    "                    [37,21,6,12,9,4,1,14,16,31,19], #“Automation Components” : hardware and software components used in industrial automation systems. (rouge centre)\n",
    "                    [33,46,8], #Product Evaluation : evaluate the quality, affordability and reliability of products and services (rouge, fin)\n",
    "                    [44,51,23,41,49,57,22], #Customer Support : Reliability and Quality in Customer Service and Support (bleu ciel)\n",
    "                    [58,59], #Quick Customer Service (marron)\n",
    "                    [38,10,26,52,39,43], #Problem Solving and Communication (focus on the importance of being efficient and precise when solving problems) (jaune)\n",
    "                    [45,47,55,53,54], #Assistance and Guidance (noir)\n",
    "                    [29,30,11,24], #Power Supply Issues (2e vert, haut)\n",
    "                    [7,5,2,25,15,34,18,28,17], #Technical Support (2e vert, bas)\n",
    "                    [48,56] #None : positive feedback (2e rouge)\n",
    "]\n",
    "\n",
    "# Set the topic names for the new aggregated topic\n",
    "# It must match the order from the topics_to_merge list\n",
    "label_names = [\n",
    "    \"Outliers\",\n",
    "    \"Automation Components\",\n",
    "    \"Technical Support\",\n",
    "    \"Delivery Deadlines\",\n",
    "    \"Problem Solving & Comm\",\n",
    "    \"Power Supply Issues\",\n",
    "    \"Customer Support\", #Reliability and Quality in Customer Service and Support\n",
    "    \"Product Evaluation\",\n",
    "    \"Pricing\", #Quotation and Pricing Strategies\n",
    "    \"Assistance\", #Assistance and Guidance\n",
    "    \"Touch Screens\", #Touch Panels and Screens\n",
    "    \"Frequency Converters\",\n",
    "    \"Positive feedback\",\n",
    "    \"Quick Customer Service\"\n",
    "    ]\n",
    "\n",
    "# Create a new merged bertopic model \n",
    "topic_model_merged = create_merged_model(docs, topic_model, topics_to_merge, label_names)\n",
    "# topic_model_merged.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify the visualize_topics_per_class method from bertopic to be able to print a barchart vertically\n",
    "# def visualize_topics_per_class_orient(topic_model,\n",
    "#                                topics_per_class: pd.DataFrame,\n",
    "#                                top_n_topics: int = 10,\n",
    "#                                topics: List[int] = None,\n",
    "#                                normalize_frequency: bool = False,\n",
    "#                                custom_labels: Union[bool, str] = False,\n",
    "#                                title: str = \"<b>Topics per Class</b>\",\n",
    "#                                width: int = 1250,\n",
    "#                                height: int = 900,\n",
    "#                                orient: str = \"h\") -> go.Figure:\n",
    "#     \"\"\" Visualize topics per class\n",
    "\n",
    "#     Arguments:\n",
    "#         topic_model: A fitted BERTopic instance.\n",
    "#         topics_per_class: The topics you would like to be visualized with the\n",
    "#                           corresponding topic representation\n",
    "#         top_n_topics: To visualize the most frequent topics instead of all\n",
    "#         topics: Select which topics you would like to be visualized\n",
    "#         normalize_frequency: Whether to normalize each topic's frequency individually\n",
    "#         custom_labels: If bool, whether to use custom topic labels that were defined using \n",
    "#                        `topic_model.set_topic_labels`.\n",
    "#                        If `str`, it uses labels from other aspects, e.g., \"Aspect1\".\n",
    "#         title: Title of the plot.\n",
    "#         width: The width of the figure.\n",
    "#         height: The height of the figure.\n",
    "#         orient: The orientation of the barchart, 'h' for horizontal, anything else for vertical\n",
    "\n",
    "#     Returns:\n",
    "#         A plotly.graph_objects.Figure including all traces\n",
    "\n",
    "#     Examples:\n",
    "\n",
    "#     To visualize the topics per class, simply run:\n",
    "\n",
    "#     ```python\n",
    "#     topics_per_class = topic_model.topics_per_class(docs, classes)\n",
    "#     topic_model.visualize_topics_per_class(topics_per_class)\n",
    "#     ```\n",
    "\n",
    "#     Or if you want to save the resulting figure:\n",
    "\n",
    "#     ```python\n",
    "#     fig = topic_model.visualize_topics_per_class(topics_per_class)\n",
    "#     fig.write_html(\"path/to/file.html\")\n",
    "#     ```\n",
    "#     <iframe src=\"../../getting_started/visualization/topics_per_class.html\"\n",
    "#     style=\"width:1400px; height: 1000px; border: 0px;\"\"></iframe>\n",
    "#     \"\"\"\n",
    "#     colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#D55E00\", \"#0072B2\", \"#CC79A7\"]\n",
    "\n",
    "#     # Select topics based on top_n and topics args\n",
    "#     freq_df = topic_model.get_topic_freq()\n",
    "#     freq_df = freq_df.loc[freq_df.Topic != -1, :]\n",
    "#     if topics is not None:\n",
    "#         selected_topics = list(topics)\n",
    "#     elif top_n_topics is not None:\n",
    "#         selected_topics = sorted(freq_df.Topic.to_list()[:top_n_topics])\n",
    "#     else:\n",
    "#         selected_topics = sorted(freq_df.Topic.to_list())\n",
    "\n",
    "#     # Prepare data\n",
    "#     if isinstance(custom_labels, str):\n",
    "#         topic_names = [[[str(topic), None]] + topic_model.topic_aspects_[custom_labels][topic] for topic in topics]\n",
    "#         topic_names = [\"_\".join([label[0] for label in labels[:4]]) for labels in topic_names]\n",
    "#         topic_names = [label if len(label) < 30 else label[:27] + \"...\" for label in topic_names]\n",
    "#         topic_names = {key: topic_names[index] for index, key in enumerate(topic_model.topic_labels_.keys())}\n",
    "#     elif topic_model.custom_labels_ is not None and custom_labels:\n",
    "#         topic_names = {key: topic_model.custom_labels_[key + topic_model._outliers] for key, _ in topic_model.topic_labels_.items()}\n",
    "#     else:\n",
    "#         topic_names = {key: value[:40] + \"...\" if len(value) > 40 else value\n",
    "#                        for key, value in topic_model.topic_labels_.items()}\n",
    "#     topics_per_class[\"Name\"] = topics_per_class.Topic.map(topic_names)\n",
    "#     data = topics_per_class.loc[topics_per_class.Topic.isin(selected_topics), :]\n",
    "\n",
    "#     # Add traces\n",
    "#     fig = go.Figure()\n",
    "#     for index, topic in enumerate(selected_topics):\n",
    "#         if index == 0:\n",
    "#             visible = True\n",
    "#         else:\n",
    "#             visible = \"legendonly\"\n",
    "#         trace_data = data.loc[data.Topic == topic, :]\n",
    "#         topic_name = trace_data.Name.values[0]\n",
    "#         words = trace_data.Words.values\n",
    "#         if normalize_frequency:\n",
    "#             x = normalize(trace_data.Frequency.values.reshape(1, -1))[0]\n",
    "#         else:\n",
    "#             x = trace_data.Frequency\n",
    "\n",
    "#         ### Old part from the source github of bertopic ###\n",
    "#         # fig.add_trace(go.Bar(y=trace_data.Class,\n",
    "#         #                      x=x,\n",
    "#         #                      visible=visible,\n",
    "#         #                      marker_color=colors[index % 7],\n",
    "#         #                      hoverinfo=\"text\",\n",
    "#         #                      name=topic_name,\n",
    "#         #                      orientation=\"h\",\n",
    "#         #                      hovertext=[f'<b>Topic {topic}</b><br>Words: {word}' for word in words]))\n",
    "        \n",
    "#         fig.add_trace(go.Bar(y=trace_data.Class if orient == \"h\" else x,\n",
    "#                             x=x if orient == \"h\" else trace_data.Class,\n",
    "#                             visible=visible,\n",
    "#                             marker_color=colors[index % 7],\n",
    "#                             hoverinfo=\"text\",\n",
    "#                             name=topic_name,\n",
    "#                             orientation=orient,\n",
    "#                             hovertext=[f'<b>Topic {topic}</b><br>Words: {word}' for word in words]))\n",
    "\n",
    "#     # Styling of the visualization\n",
    "#     fig.update_xaxes(showgrid=True)\n",
    "#     fig.update_yaxes(showgrid=True)\n",
    "#     fig.update_layout(\n",
    "#         xaxis_title=\"Normalized Frequency\" if normalize_frequency else \"Frequency\",\n",
    "#         yaxis_title=\"Class\",\n",
    "#         title={\n",
    "#             'text': f\"{title}\",\n",
    "#             'y': .95,\n",
    "#             'x': 0.40,\n",
    "#             'xanchor': 'center',\n",
    "#             'yanchor': 'top',\n",
    "#             'font': dict(\n",
    "#                 size=22,\n",
    "#                 color=\"Black\")\n",
    "#         },\n",
    "#         template=\"simple_white\",\n",
    "#         width=width,\n",
    "#         height=height,\n",
    "#         hoverlabel=dict(\n",
    "#             bgcolor=\"white\",\n",
    "#             font_size=16,\n",
    "#             font_family=\"Rockwell\"\n",
    "#         ),\n",
    "#         legend=dict(\n",
    "#             title=\"<b>Global Topic Representation\",\n",
    "#         )\n",
    "#     )\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics_per_class_df(df, bertopic_model, classes_column, filter=False, filter_group=None, filter_value=None,  sortedBy=None, ascending=True):\n",
    "    \"\"\"\n",
    "    Create a dataframe representing the topics per class.\n",
    "\n",
    "    This function takes as input a dataframe `df`, a BERTopic model `bertopic_model`, a column name `classes_column` representing the classes, an optional boolean parameter `filter` which determines whether to filter the data based on a subclass, an optional parameter `filter_group` representing the column name of the subclass to filter on, an optional parameter `filter_value` representing the value of the subclass to filter on, an optional parameter `sortedBy` which can be used to sort the topics by either \"Frequency\" or \"Name\", and an optional boolean parameter `ascending` which determines the sorting order (ascending or descending).\n",
    "\n",
    "    The function first checks that the values of `sortedBy`, `ascending`, and `subclass_name` and `subclass_value` (if `filter` is `True`) are valid. If any of these values are not valid, a ValueError is raised with an appropriate error message.\n",
    "\n",
    "    Then, depending on whether `filter` is `True` or not, the function either calculates the topics per class using the `topics_per_class` method of the BERTopic model or calls a nested function `topics_per_subclass` to compute the topics per class with filtering.\n",
    "\n",
    "    If `sortedBy` is \"Frequency\", the resulting dataframe is sorted by frequency in either ascending or descending order depending on the value of `ascending`.\n",
    "\n",
    "    The resulting dataframe is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input dataframe containing the data to be used.\n",
    "        bertopic_model (BERTopic): The BERTopic model used to calculate the topics per class.\n",
    "        classes_column (str): The name of the column in `df` representing the classes.\n",
    "        filter (bool): An optional boolean parameter used to determine whether to filter the data based on a subclass. Defaults to False.\n",
    "        filter_group (str): An optional parameter representing the column name of the subclass to filter on. Only used if `filter` is True. Defaults to None.\n",
    "        filter_value: An optional parameter representing the value of the subclass to filter on. Only used if `filter` is True. Defaults to None.\n",
    "        sortedBy (str): An optional parameter used to sort the topics by either \"Frequency\" or \"Name\". Defaults to None.\n",
    "        ascending (bool): An optional boolean parameter used to determine the sorting order. If True, sorts in ascending order. If False, sorts in descending order. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe representing the topics per class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that the value of sortedBy is valid\n",
    "    if sortedBy not in [None, \"Frequency\", \"Name\"]:\n",
    "        raise ValueError(\"sortedBy must be either None (default value), 'Frequency', or 'Name'\")\n",
    "    # Check that ascending is only used if sortedBy is not None\n",
    "    if sortedBy is None and ascending != True:\n",
    "        raise ValueError(\"ascending can only be used if sortedBy parameter is used\")\n",
    "    # Check that the value of ascending is valid\n",
    "    if ascending not in [True, False]:\n",
    "        raise ValueError(\"ascending must be either True or False\")\n",
    "    # Check that subclass_name and subclass_value are provided if filter is True\n",
    "    if filter and (filter_group is None or filter_value is None):\n",
    "        raise ValueError(\"If filter is True, both subclass_name and subclass_value must be provided\")\n",
    "    \n",
    "    # Define a function to compute topics_per_class with filtering\n",
    "    def topics_per_subclass():\n",
    "        \"\"\"\n",
    "        Create a dataframe that contains the topic number, the list of words that describe the topic,\n",
    "        and the frequency of documents from this topic that belong to the element from the first \"Class\" column\n",
    "        for a subset of data that is filtered by a given subclass value.\n",
    "        Basically it does the same as topics_per_class method from bertopic adding a filter that depends on an other class\n",
    "\n",
    "        :param df: A pandas DataFrame containing the full data\n",
    "        :param bertopic_model: A bertopic model used to compute the topics\n",
    "        :param classes_column: The name of the column in df that contains the class values\n",
    "        :param filter_group: The name of the column in df that contains the subclass values (the filter values)\n",
    "        :param filter_value: The value of the subclass to filter the data by\n",
    "        :return: A pandas DataFrame containing the topic number, the list of words that describe the topic,\n",
    "                and the frequency of documents from this topic that belong to the element from the first \"Class\" column\n",
    "                for the filtered data (subclass data)\n",
    "        \"\"\"\n",
    "        # Filter your data based on the values from the chosen subclass\n",
    "        filtered_data = df[df[filter_group] == filter_value]\n",
    "        classes_filtered_data=filtered_data[classes_column].astype(str).tolist()\n",
    "        filtered_topics = [bertopic_model.topics_[i] for i in filtered_data.index.tolist()]\n",
    "\n",
    "        # Create manually a topic_per_class dataframe from a subset of the full documents\n",
    "        topics_per_subClass_df = pd.DataFrame({'Topic': filtered_topics, 'Class': classes_filtered_data})\n",
    "        # Calculate the frequency of each topic for each class\n",
    "        topics_per_subClass_df = topics_per_subClass_df.groupby(['Topic', 'Class']).size().reset_index(name='Frequency')\n",
    "        # Add the words that describe each topic\n",
    "        topic_words = {row['Topic']: row['Name'] for _, row in bertopic_model.get_topic_info().iterrows()}\n",
    "        topics_per_subClass_df['Words'] = topics_per_subClass_df['Topic'].map(topic_words)\n",
    "\n",
    "        # Add rows for missing topics with a frequency of 0\n",
    "        missing_topics = set(bertopic_model.get_topics().keys()) - set(topics_per_subClass_df['Topic'].unique())\n",
    "        for topic in missing_topics:\n",
    "            for class_ in topics_per_subClass_df['Class'].unique():\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Topic': [topic],\n",
    "                    'Words': [topic_words[topic]],\n",
    "                    'Frequency': [0],\n",
    "                    'Class': [class_]\n",
    "                })\n",
    "                topics_per_subClass_df = pd.concat([topics_per_subClass_df, new_row], ignore_index=True)\n",
    "        \n",
    "        return topics_per_subClass_df\n",
    "\n",
    "    if filter:\n",
    "        topics_per_class = topics_per_subclass()\n",
    "    else:\n",
    "        topics_per_class = bertopic_model.topics_per_class(df[\"processed_data\"].astype(str).tolist(), classes=df[classes_column].to_list())\n",
    "\n",
    "    if sortedBy==\"Frequency\":\n",
    "        topics_per_class = topics_per_class.sort_values(by='Frequency', ascending=ascending)\n",
    "    \n",
    "    return topics_per_class\n",
    "\n",
    "def visualize_topics_per_class_options(topic_model, topics_per_class, orient=\"h\", **kwargs):\n",
    "    \"\"\"\n",
    "    Visualize the topic representation of major topics per class.\n",
    "\n",
    "    This function takes as input a dataframe `df`, a topic model `topic_model`, a column name `classes_column` representing the classes, an orientation `orient` which can be either horizontal (\"h\") or vertical (\"v\"), an optional parameter `sortedBy` which can be used to sort the topics by either \"Frequency\" or \"Name\", an optional parameter `ascending` which determines the sorting order (ascending or descending), and additional keyword arguments `**kwargs`.\n",
    "\n",
    "    The function first calculates the topics per class using the `topics_per_class` method of the topic model, which takes as input the processed data from the dataframe and the classes. Then, depending on the specified orientation and sorting options, it either uses the `visualize_topics_per_class` method of the topic model (if the orientation is horizontal) or a modified version of this method called `visualize_topics_per_class2` (if the orientation is vertical) to create a figure representing the topics per class. The figure is then returned.\n",
    "\n",
    "    This function allows you to easily visualize the distribution of topics per class in your data using a topic model.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input dataframe containing the data to be visualized.\n",
    "        topic_model (TopicModel): The topic model used to calculate the topics per class.\n",
    "        classes_column (str): The name of the column in `df` representing the classes.\n",
    "        orient (str): The orientation of the visualization. Can be either \"h\" for horizontal or \"v\" for vertical. Defaults to \"h\".\n",
    "        sortedBy (str): An optional parameter used to sort the topics by either \"Frequency\" or \"Name\". Defaults to None.\n",
    "        ascending (bool): An optional parameter used to determine the sorting order. If True, sorts in ascending order. If False, sorts in descending order. Defaults to True.\n",
    "        **kwargs: Additional keyword arguments passed to the visualization method.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure representing the topics per class.\n",
    "    \"\"\"\n",
    "    # Modify the visualize_topics_per_class method from bertopic to be able to print a barchart vertically\n",
    "    def visualize_topics_per_class_orient(topic_model,\n",
    "                                topics_per_class: pd.DataFrame,\n",
    "                                top_n_topics: int = 10,\n",
    "                                topics: List[int] = None,\n",
    "                                normalize_frequency: bool = False,\n",
    "                                custom_labels: Union[bool, str] = False,\n",
    "                                title: str = \"<b>Topics per Class</b>\",\n",
    "                                width: int = 1250,\n",
    "                                height: int = 900,\n",
    "                                orient: str = \"h\") -> go.Figure:\n",
    "        \"\"\" Visualize topics per class\n",
    "\n",
    "        Arguments:\n",
    "            topic_model: A fitted BERTopic instance.\n",
    "            topics_per_class: The topics you would like to be visualized with the\n",
    "                            corresponding topic representation\n",
    "            top_n_topics: To visualize the most frequent topics instead of all\n",
    "            topics: Select which topics you would like to be visualized\n",
    "            normalize_frequency: Whether to normalize each topic's frequency individually\n",
    "            custom_labels: If bool, whether to use custom topic labels that were defined using \n",
    "                        `topic_model.set_topic_labels`.\n",
    "                        If `str`, it uses labels from other aspects, e.g., \"Aspect1\".\n",
    "            title: Title of the plot.\n",
    "            width: The width of the figure.\n",
    "            height: The height of the figure.\n",
    "            orient: The orientation of the barchart, 'h' for horizontal, anything else for vertical\n",
    "\n",
    "        Returns:\n",
    "            A plotly.graph_objects.Figure including all traces\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        To visualize the topics per class, simply run:\n",
    "\n",
    "        ```python\n",
    "        topics_per_class = topic_model.topics_per_class(docs, classes)\n",
    "        topic_model.visualize_topics_per_class(topics_per_class)\n",
    "        ```\n",
    "\n",
    "        Or if you want to save the resulting figure:\n",
    "\n",
    "        ```python\n",
    "        fig = topic_model.visualize_topics_per_class(topics_per_class)\n",
    "        fig.write_html(\"path/to/file.html\")\n",
    "        ```\n",
    "        <iframe src=\"../../getting_started/visualization/topics_per_class.html\"\n",
    "        style=\"width:1400px; height: 1000px; border: 0px;\"\"></iframe>\n",
    "        \"\"\"\n",
    "        colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#D55E00\", \"#0072B2\", \"#CC79A7\"]\n",
    "\n",
    "        # Select topics based on top_n and topics args\n",
    "        freq_df = topic_model.get_topic_freq()\n",
    "        freq_df = freq_df.loc[freq_df.Topic != -1, :]\n",
    "        if topics is not None:\n",
    "            selected_topics = list(topics)\n",
    "        elif top_n_topics is not None:\n",
    "            selected_topics = sorted(freq_df.Topic.to_list()[:top_n_topics])\n",
    "        else:\n",
    "            selected_topics = sorted(freq_df.Topic.to_list())\n",
    "\n",
    "        # Prepare data\n",
    "        if isinstance(custom_labels, str):\n",
    "            topic_names = [[[str(topic), None]] + topic_model.topic_aspects_[custom_labels][topic] for topic in topics]\n",
    "            topic_names = [\"_\".join([label[0] for label in labels[:4]]) for labels in topic_names]\n",
    "            topic_names = [label if len(label) < 30 else label[:27] + \"...\" for label in topic_names]\n",
    "            topic_names = {key: topic_names[index] for index, key in enumerate(topic_model.topic_labels_.keys())}\n",
    "        elif topic_model.custom_labels_ is not None and custom_labels:\n",
    "            topic_names = {key: topic_model.custom_labels_[key + topic_model._outliers] for key, _ in topic_model.topic_labels_.items()}\n",
    "        else:\n",
    "            topic_names = {key: value[:40] + \"...\" if len(value) > 40 else value\n",
    "                        for key, value in topic_model.topic_labels_.items()}\n",
    "        topics_per_class[\"Name\"] = topics_per_class.Topic.map(topic_names)\n",
    "        data = topics_per_class.loc[topics_per_class.Topic.isin(selected_topics), :]\n",
    "\n",
    "        # Add traces\n",
    "        fig = go.Figure()\n",
    "        for index, topic in enumerate(selected_topics):\n",
    "            if index == 0:\n",
    "                visible = True\n",
    "            else:\n",
    "                visible = \"legendonly\"\n",
    "            trace_data = data.loc[data.Topic == topic, :]\n",
    "            topic_name = trace_data.Name.values[0]\n",
    "            words = trace_data.Words.values\n",
    "            if normalize_frequency:\n",
    "                x = normalize(trace_data.Frequency.values.reshape(1, -1))[0]\n",
    "            else:\n",
    "                x = trace_data.Frequency\n",
    "\n",
    "            ### Old part from the source github of bertopic ###\n",
    "            # fig.add_trace(go.Bar(y=trace_data.Class,\n",
    "            #                      x=x,\n",
    "            #                      visible=visible,\n",
    "            #                      marker_color=colors[index % 7],\n",
    "            #                      hoverinfo=\"text\",\n",
    "            #                      name=topic_name,\n",
    "            #                      orientation=\"h\",\n",
    "            #                      hovertext=[f'<b>Topic {topic}</b><br>Words: {word}' for word in words]))\n",
    "            \n",
    "            fig.add_trace(go.Bar(y=trace_data.Class if orient == \"h\" else x,\n",
    "                                x=x if orient == \"h\" else trace_data.Class,\n",
    "                                visible=visible,\n",
    "                                marker_color=colors[index % 7],\n",
    "                                hoverinfo=\"text\",\n",
    "                                name=topic_name,\n",
    "                                orientation=orient,\n",
    "                                hovertext=[f'<b>Topic {topic}</b><br>Words: {word}' for word in words]))\n",
    "\n",
    "        # Styling of the visualization\n",
    "        fig.update_xaxes(showgrid=True)\n",
    "        fig.update_yaxes(showgrid=True)\n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Normalized Frequency\" if normalize_frequency else \"Frequency\",\n",
    "            yaxis_title=\"Class\",\n",
    "            title={\n",
    "                'text': f\"{title}\",\n",
    "                'y': .95,\n",
    "                'x': 0.40,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(\n",
    "                    size=22,\n",
    "                    color=\"Black\")\n",
    "            },\n",
    "            template=\"simple_white\",\n",
    "            width=width,\n",
    "            height=height,\n",
    "            hoverlabel=dict(\n",
    "                bgcolor=\"white\",\n",
    "                font_size=16,\n",
    "                font_family=\"Rockwell\"\n",
    "            ),\n",
    "            legend=dict(\n",
    "                title=\"<b>Global Topic Representation\",\n",
    "            )\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    if orient==\"h\":\n",
    "        # using the source method to do it, without having the possibility to choose the orient (at the date of 09/2023)\n",
    "        # we keep the use of the source method even if we could use only the new visualize_topics_per_class_orient. Because it permits to know and enjoy the modifications done in visualize_topics_per_class in the future by the owner of this source code\n",
    "        fig = topic_model.visualize_topics_per_class(topics_per_class, **kwargs)\n",
    "    else:  \n",
    "        # using the modified source method to do it, adding the option to print the chart vertically\n",
    "        fig = visualize_topics_per_class_orient(topic_model, topics_per_class, orient=orient, **kwargs)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_chart_per_class(\n",
    "                            df, \n",
    "                            bertopic_model, \n",
    "                            classes_column, \n",
    "                            filter=False, \n",
    "                            filter_group=None, \n",
    "                            filter_value=None, \n",
    "                            sortedBy=None, \n",
    "                            ascending=True, \n",
    "                            orient=\"h\", \n",
    "                            **kwargs):\n",
    "    \"\"\"\n",
    "    Create a chart representing the topics per class.\n",
    "\n",
    "    This function takes as input a dataframe `df`, a BERTopic model `bertopic_model`, a column name `classes_column` representing the classes, an optional boolean parameter `filter` which determines whether to filter the data based on a subclass, an optional parameter `subclass_name` representing the column name of the subclass to filter on, an optional parameter `subclass_value` representing the value of the subclass to filter on, an optional parameter `sortedBy` which can be used to sort the topics by either \"Frequency\" or \"Name\", an optional boolean parameter `ascending` which determines the sorting order (ascending or descending), an orientation `orient` which can be either horizontal (\"h\") or vertical (\"v\"), and additional keyword arguments `**kwargs`.\n",
    "\n",
    "    The function first calls the `create_topics_per_class_df` function to create a dataframe representing the topics per class. Then, it calls the `visualize_topics_per_class_options` function to create a chart from this dataframe. The resulting chart is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input dataframe containing the data to be visualized.\n",
    "        bertopic_model (BERTopic): The BERTopic model used to calculate the topics per class.\n",
    "        classes_column (str): The name of the column in `df` representing the classes.\n",
    "        filter (bool): An optional boolean parameter used to determine whether to filter the data based on a subclass. Defaults to False.\n",
    "        filter_group (str): An optional parameter representing the column name of the subclass to filter on. Only used if `filter` is True. Defaults to None.\n",
    "        filter_value: An optional parameter representing the value of the subclass to filter on. Only used if `filter` is True. Defaults to None.\n",
    "        sortedBy (str): An optional parameter used to sort the topics by either \"Frequency\" or \"Name\". Defaults to None.\n",
    "        ascending (bool): An optional boolean parameter used to determine the sorting order. If True, sorts in ascending order. If False, sorts in descending order. Defaults to True.\n",
    "        orient (str): The orientation of the visualization. Can be either \"h\" for horizontal or \"v\" for vertical. Defaults to \"h\".\n",
    "        **kwargs: Additional keyword arguments passed to the visualization method.\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objs.Figure: The resulting chart representing the topics per class.\n",
    "    \"\"\"\n",
    "    topics_per_class = create_topics_per_class_df(df, bertopic_model, classes_column, filter=filter, subclass_name=filter_group, subclass_value=filter_value, sortedBy=sortedBy, ascending=ascending)\n",
    "    fig = visualize_topics_per_class_options(bertopic_model, topics_per_class, orient=orient, **kwargs)\n",
    "\n",
    "    return fig \n",
    "\n",
    "def save_graph_html(fig, path, name):\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    return fig.write_html(path+\"/\"+name+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m### Graph barchart topics by sentiment\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# Code without using the create_chart_per_class wrapper function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39m# Code using the create_chart_per_class wrapper function\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m fig \u001b[39m=\u001b[39m create_chart_per_class(df4, topic_model_merged, \u001b[39m\"\u001b[39;49m\u001b[39msentiment_label\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     11\u001b[0m                              sortedBy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFrequency\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     12\u001b[0m                              orient\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     13\u001b[0m                              custom_labels\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     14\u001b[0m                              title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTopics per Sentiment\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     15\u001b[0m                              width\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m750\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Save the figure as an HTML file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# save_graph_html(fig, \"../data/graphs\", \"topic_merged_per_class_sentiment\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m fig\n",
      "Cell \u001b[0;32mIn[158], line 300\u001b[0m, in \u001b[0;36mcreate_chart_per_class\u001b[0;34m(df, bertopic_model, classes_column, filter, subclass_name, subclass_value, sortedBy, ascending, orient, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_chart_per_class\u001b[39m(\n\u001b[1;32m    268\u001b[0m                             df, \n\u001b[1;32m    269\u001b[0m                             bertopic_model, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m                             orient\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mh\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    277\u001b[0m                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m    Create a chart representing the topics per class.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39m        plotly.graph_objs.Figure: The resulting chart representing the topics per class.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     topics_per_class \u001b[39m=\u001b[39m create_topics_per_class_df(df, bertopic_model, classes_column, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m, subclass_name\u001b[39m=\u001b[39;49msubclass_name, subclass_value\u001b[39m=\u001b[39;49msubclass_value, sortedBy\u001b[39m=\u001b[39;49msortedBy, ascending\u001b[39m=\u001b[39;49mascending)\n\u001b[1;32m    301\u001b[0m     fig \u001b[39m=\u001b[39m visualize_topics_per_class_options(bertopic_model, topics_per_class, orient\u001b[39m=\u001b[39morient, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m fig\n",
      "Cell \u001b[0;32mIn[158], line 89\u001b[0m, in \u001b[0;36mcreate_topics_per_class_df\u001b[0;34m(df, bertopic_model, classes_column, filter, subclass_name, subclass_value, sortedBy, ascending)\u001b[0m\n\u001b[1;32m     87\u001b[0m     topics_per_class \u001b[39m=\u001b[39m topics_per_subclass()\n\u001b[1;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     topics_per_class \u001b[39m=\u001b[39m bertopic_model\u001b[39m.\u001b[39;49mtopics_per_class(df[\u001b[39m\"\u001b[39;49m\u001b[39mprocessed_data\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mstr\u001b[39;49m)\u001b[39m.\u001b[39;49mtolist(), classes\u001b[39m=\u001b[39;49mdf[classes_column]\u001b[39m.\u001b[39;49mto_list())\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m sortedBy\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFrequency\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     92\u001b[0m     topics_per_class \u001b[39m=\u001b[39m topics_per_class\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFrequency\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39mascending)\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/_bertopic.py:862\u001b[0m, in \u001b[0;36mBERTopic.topics_per_class\u001b[0;34m(self, docs, classes, global_tuning)\u001b[0m\n\u001b[1;32m    859\u001b[0m     c_tf_idf \u001b[39m=\u001b[39m (global_c_tf_idf[documents_per_topic\u001b[39m.\u001b[39mTopic\u001b[39m.\u001b[39mvalues \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outliers] \u001b[39m+\u001b[39m c_tf_idf) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m\n\u001b[1;32m    861\u001b[0m \u001b[39m# Extract the words per topic\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m words_per_topic \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_words_per_topic(words, selection, c_tf_idf, calculate_aspects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    863\u001b[0m topic_frequency \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(documents_per_topic\u001b[39m.\u001b[39mClass\u001b[39m.\u001b[39mvalues,\n\u001b[1;32m    864\u001b[0m                             index\u001b[39m=\u001b[39mdocuments_per_topic\u001b[39m.\u001b[39mTopic)\u001b[39m.\u001b[39mto_dict()\n\u001b[1;32m    866\u001b[0m \u001b[39m# Fill dataframe with results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/_bertopic.py:3568\u001b[0m, in \u001b[0;36mBERTopic._extract_words_per_topic\u001b[0;34m(self, words, documents, c_tf_idf, calculate_aspects)\u001b[0m\n\u001b[1;32m   3566\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model, \u001b[39mlist\u001b[39m):\n\u001b[1;32m   3567\u001b[0m     \u001b[39mfor\u001b[39;00m tuner \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model:\n\u001b[0;32m-> 3568\u001b[0m         topics \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39;49mextract_topics(\u001b[39mself\u001b[39;49m, documents, c_tf_idf, topics)\n\u001b[1;32m   3569\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model, BaseRepresentation):\n\u001b[1;32m   3570\u001b[0m     topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model\u001b[39m.\u001b[39mextract_topics(\u001b[39mself\u001b[39m, documents, c_tf_idf, topics)\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/representation/_keybert.py:91\u001b[0m, in \u001b[0;36mKeyBERTInspired.extract_topics\u001b[0;34m(self, topic_model, documents, c_tf_idf, topics)\u001b[0m\n\u001b[1;32m     87\u001b[0m topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_candidate_words(topic_model, c_tf_idf, topics)\n\u001b[1;32m     89\u001b[0m \u001b[39m# We calculate the similarity between word and document embeddings and create\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m# topic embeddings from the representative document embeddings\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m sim_matrix, words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_embeddings(topic_model, topics, representative_docs, repr_doc_indices)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Find the best matching words based on the similarity matrix for each topic\u001b[39;00m\n\u001b[1;32m     94\u001b[0m updated_topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_top_words(words, topics, sim_matrix)\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/representation/_keybert.py:163\u001b[0m, in \u001b[0;36mKeyBERTInspired._extract_embeddings\u001b[0;34m(self, topic_model, topics, representative_docs, repr_doc_indices)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Extract the representative document embeddings and create topic embeddings.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mThen extract word embeddings and calculate the cosine similarity between topic\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39membeddings and the word embeddings. Topic embeddings are the average of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39m    vocab: The complete vocabulary of input documents\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# Calculate representative docs embeddings and create topic embeddings\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m repr_embeddings \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39;49m_extract_embeddings(representative_docs, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    164\u001b[0m topic_embeddings \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mmean(repr_embeddings[i[\u001b[39m0\u001b[39m]:i[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m repr_doc_indices]\n\u001b[1;32m    166\u001b[0m \u001b[39m# Calculate word embeddings and extract best matching with updated topic_embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/_bertopic.py:3126\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[0;34m(self, documents, images, method, verbose)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model\u001b[39m.\u001b[39membed_words(words\u001b[39m=\u001b[39mdocuments, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m   3125\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 3126\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_model\u001b[39m.\u001b[39;49membed_documents(documents, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m   3127\u001b[0m \u001b[39melif\u001b[39;00m documents[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMake sure to use an embedding model that can either embed documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3129\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mor images depending on which you want to embed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/backend/_base.py:69\u001b[0m, in \u001b[0;36mBaseEmbedder.embed_documents\u001b[0;34m(self, document, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_documents\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     56\u001b[0m                     document: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     57\u001b[0m                     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(document, verbose)\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/backend/_sentencetransformers.py:65\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     52\u001b[0m           documents: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     53\u001b[0m           verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     54\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_model\u001b[39m.\u001b[39;49mencode(documents, show_progress_bar\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:162\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    160\u001b[0m sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m    161\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(sentences_batch)\n\u001b[0;32m--> 162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/sentence_transformers/util.py:300\u001b[0m, in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, target_device)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch[key], Tensor):\n\u001b[0;32m--> 300\u001b[0m         batch[key] \u001b[39m=\u001b[39m batch[key]\u001b[39m.\u001b[39;49mto(target_device)\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "### Graph barchart topics by sentiment\n",
    "\n",
    "# Code without using the create_chart_per_class wrapper function\n",
    "# create topics_per_class df in order to run the visualization then.\n",
    "# topics_per_class = create_topics_per_class_df(df4, topic_model_merged, \"sentiment_label\", sortedBy=\"Frequency\")\n",
    "# visualize the topic representation of major topics per Sentiment (positive, negative, neutral):\n",
    "# fig = visualize_topics_per_class_options(topic_model_merged, topics_per_class, orient=\"v\", custom_labels=True, title=\"Topics per Sentiment\", width=500, height=750)\n",
    "\n",
    "# Code using the create_chart_per_class wrapper function\n",
    "fig = create_chart_per_class(df4, topic_model_merged, \"sentiment_label\", \n",
    "                             sortedBy=\"Frequency\", \n",
    "                             orient=\"v\", \n",
    "                             custom_labels=True, \n",
    "                             title=\"Topics per Sentiment\", \n",
    "                             width=500, height=750)\n",
    "\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_merged_per_class_sentiment\")\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph barchart topics by emotion\n",
    "\n",
    "# visualize the topic representation of major topics per Emotions (28 different emotions):\n",
    "fig = create_chart_per_class(df4, topic_model_merged, \"single_emotion_label\", \n",
    "                             custom_labels=True, \n",
    "                             title=\"Topics per Emotion\", \n",
    "                             width=1150, height=750)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_merged_per_class_emotion\")\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph barchart topics by year\n",
    "\n",
    "# visualize the topic representation of major topics per Year:\n",
    "fig = create_chart_per_class(df4, topic_model_merged, \"year\", \n",
    "                             custom_labels=True, \n",
    "                             title=\"Topics per Year\", \n",
    "                             width=1400, height=750)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_merged_per_class_year\")\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph barchart topics by \"by_class_name\" parameter for each value from \"subclass_names\"\n",
    "\n",
    "# Create a dictionary that match column name to a shorter name for saving name file\n",
    "shorter_names={\n",
    "    \"single_emotion_label\":\"emotion\",\n",
    "    \"sentiment_label\":\"sentiment\",\n",
    "    \"Market Segment\":\"market\",\n",
    "    \"Account Country\":\"country\",\n",
    "    \"Zone\":\"zone\",\n",
    "    \"Clusters\":\"cluster\",\n",
    "    \"year\":\"year\"\n",
    "}\n",
    "\n",
    "subclass_name = \"single_emotion_label\"\n",
    "subclass_name_shorter = shorter_names[subclass_name]\n",
    "by_class_name = \"year\"\n",
    "# Loop for each value from the subclass\n",
    "for subclass_value in df4[subclass_name].unique() :\n",
    "    # visualize the topic representation of major topics per by_class_name for each value from subclass_name:\n",
    "    fig = create_chart_per_class(df4, topic_model_merged, by_class_name,\n",
    "                                filter=True, \n",
    "                                subclass_name=subclass_name, subclass_value=subclass_value, \n",
    "                                sortedBy=\"Frequency\",\n",
    "                                custom_labels=True, \n",
    "                                title=f\"Topics per {shorter_names[by_class_name]} - {subclass_name_shorter}: {subclass_value}\", \n",
    "                                width=1400, height=750)\n",
    "\n",
    "    # Save the figure as an HTML file\n",
    "    path = f\"../data/graphs/topics_per_class/topic_model_merged/per_{shorter_names[by_class_name]}/for_{subclass_name_shorter}\"\n",
    "    save_graph_html(fig, path, subclass_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph barchart for each topic with their top 5 keywords\n",
    "\n",
    "# create barchart for each of the top_n_topics best topics\n",
    "fig = topic_model.visualize_barchart(top_n_topics=12)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topics_barchart_viz\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph hierarchical topics\n",
    "\n",
    "bertopic_model = topic_model\n",
    "hierarchical_topics = bertopic_model.hierarchical_topics(docs)\n",
    "fig = bertopic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics, custom_labels=True, width=1200, height=950)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_hierarchy\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph visualization of the documents in space\n",
    "\n",
    "# Run the visualization with the original embeddings\n",
    "topic_model_merged.visualize_documents(docs, embeddings=embeddings, custom_labels=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph visualization of the documents after reducing embeddings in space\n",
    "\n",
    "# Reduce dimensionality of embeddings, this step is optional\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "fig=topic_model_merged.visualize_documents(docs, reduced_embeddings=reduced_embeddings, custom_labels=True, hide_annotations=True)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_merged_visualize_reduced_docs\")\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph visualization of the topics over time\n",
    "\n",
    "bertopic_model = topic_model_merged\n",
    "topics_over_time = bertopic_model.topics_over_time(docs, df4['year_month'])#, nr_bins=20)\n",
    "fig = bertopic_model.visualize_topics_over_time(topics_over_time, custom_labels=True)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_merged_topics_over_time_months\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph heatmap\n",
    "\n",
    "fig = topic_model_merged.visualize_heatmap(custom_labels=True, n_clusters=4, height=600)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_merged_heatmap\")\n",
    "\n",
    "fig2 = topic_model.visualize_heatmap(n_clusters=8,  height=750)\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig2, \"../data/graphs\", \"topic_heatmap\")\n",
    "\n",
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph Visualize topics in 2D space\n",
    "\n",
    "fig = topic_model.visualize_topics()\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig, \"../data/graphs\", \"topic_visualize_topics\")\n",
    "\n",
    "fig2 = topic_model_merged.visualize_topics()\n",
    "# Save the figure as an HTML file\n",
    "save_graph_html(fig2, \"../data/graphs\", \"topic_merged_visualize_topics\")\n",
    "\n",
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph Topic distribution for one specific document\n",
    "\n",
    "topic_distr, topic_token_distr = topic_model_merged.approximate_distribution(docs, calculate_tokens=True)\n",
    "topic_model_merged.visualize_distribution(topic_distr[1], custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph Topic distribution by keywords for one specific document\n",
    "\n",
    "bertopic_model = topic_model_merged\n",
    "topic_distr, topic_token_distr = bertopic_model.approximate_distribution(docs, calculate_tokens=True, window=4)\n",
    "# # Visualize the token-level distributions\n",
    "bertopic_model.visualize_approximate_distribution(docs[1], topic_token_distr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the vocabulary using the OOP\n",
    "\n",
    "# import nltk\n",
    "# import sys\n",
    "# sys.path.insert(0, '..')\n",
    "# from utils import ngrams_list, more_stopwords, keybert_kwargs\n",
    "# from vocabulary.vocabulary import VocabularyCreator\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# stopwords.extend(more_stopwords)\n",
    "\n",
    "# vocabulary_creator = VocabularyCreator(ngrams_list, **keybert_kwargs)\n",
    "# vocabulary_list = vocabulary_creator.keybert_vocabulary(df4)\n",
    "# bertopic_vectorizer = CountVectorizer(vocabulary=vocabulary_list, stop_words=stopwords, lowercase=True, ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cattiaux/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/cattiaux/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/cattiaux/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Wordcloud \n",
    "# Lemmatize keywords and recompute their probabilities for the wordcloud image\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def lemmatize_words(topic_words):\n",
    "    \"\"\"\n",
    "    Lemmatize the words and combine their probabilities.\n",
    "\n",
    "    This function takes as input a list of tuples `topic_words`, where each tuple contains a word and its probability. The function lemmatizes each word using the WordNetLemmatizer from the NLTK library, and combines the probabilities of the lemmas and their inflected forms.\n",
    "\n",
    "    The resulting dictionary, where the keys are the lemmas and the values are their combined probabilities, is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        topic_words (list): A list of tuples, where each tuple contains a word (str) and its probability (float).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the lemmas (str) and the values are their combined probabilities (float).\n",
    "    \"\"\"\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        Convert NLTK part of speech tags to WordNet tags.\n",
    "\n",
    "        This function takes as input a part of speech tag in the format used by the NLTK library and returns the corresponding WordNet tag. The mapping between NLTK and WordNet tags is as follows:\n",
    "        - 'J' (adjective) maps to `wordnet.ADJ`\n",
    "        - 'V' (verb) maps to `wordnet.VERB`\n",
    "        - 'N' (noun) maps to `wordnet.NOUN`\n",
    "        - 'R' (adverb) maps to `wordnet.ADV`\n",
    "        - All other tags map to `wordnet.NOUN`\n",
    "\n",
    "        Parameters:\n",
    "            treebank_tag (str): The NLTK part of speech tag to be converted.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding WordNet part of speech tag.\n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    # Create a lemmatizer object\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Create a dictionary to store the lemmas and their probabilities\n",
    "    lemma_prob = {}\n",
    "\n",
    "    # Lemmatize each word and combine their probabilities\n",
    "    for word, prob in topic_words:\n",
    "        # Tokenize the word and get its part of speech\n",
    "        tokens = nltk.word_tokenize(word)\n",
    "        pos = nltk.pos_tag(tokens)[0][1]\n",
    "        # Get the WordNet part of speech tag\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        # Lemmatize the word\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        \n",
    "        # Combine the probabilities of the lemma and its inflected forms\n",
    "        if lemma in lemma_prob:\n",
    "            lemma_prob[lemma] += prob\n",
    "        else:\n",
    "            lemma_prob[lemma] = prob\n",
    "    \n",
    "    return lemma_prob\n",
    "\n",
    "def recalculate_probabilities(lemma_prob, docs, topic_model):\n",
    "    \"\"\"\n",
    "    Recalculate the c-TF-IDF scores for the lemmas.\n",
    "\n",
    "    This function takes as input a dictionary `lemma_prob` containing the lemmas and their probabilities, a list of documents `docs`, and a BERTopic model `topic_model`. The function recalculates the c-TF-IDF scores for the lemmas using the provided documents and BERTopic model.\n",
    "\n",
    "    The function first calculates the term frequencies for each lemma using the CountVectorizer from the BERTopic model. Then, it calculates the inverse document frequencies for each lemma and uses these values to compute the c-TF-IDF scores. The c-TF-IDF scores are then normalized and used to update the probabilities of the lemmas.\n",
    "\n",
    "    The resulting dictionary, where the keys are the lemmas and the values are their updated probabilities, is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        lemma_prob (dict): A dictionary where the keys are the lemmas (str) and the values are their probabilities (float).\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        topic_model (BERTopic): The BERTopic model used to calculate the c-TF-IDF scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the lemmas (str) and the values are their updated probabilities (float).\n",
    "    \"\"\"\n",
    "    # Calculate the term frequencies using the provided CountVectorizer\n",
    "    X = topic_model.vectorizer_model.transform(docs)\n",
    "    \n",
    "    # Calculate the term frequencies for each lemma\n",
    "    tf = {}\n",
    "    for lemma, prob in lemma_prob.items():\n",
    "        index = topic_model.vectorizer_model.vocabulary_.get(lemma)\n",
    "        if index is not None:\n",
    "            tf[lemma] = np.sum(X[:, index])\n",
    "\n",
    "    # Calculate the inverse document frequencies for each lemma\n",
    "    df = np.sum(X > 0, axis=0)\n",
    "    N = X.shape[0]\n",
    "    idf = np.log(N / (df + 1))\n",
    "\n",
    "    # Calculate the c-TF-IDF scores for each lemma and normalize them\n",
    "    c_tf_idf = {}\n",
    "    for lemma, prob in lemma_prob.items():\n",
    "        index = topic_model.vectorizer_model.vocabulary_.get(lemma)\n",
    "        if index is not None:\n",
    "            c_tf_idf[lemma] = tf[lemma] * idf[0, index]\n",
    "    c_tf_idf_sum = np.sum(list(c_tf_idf.values()))\n",
    "    for lemma, score in c_tf_idf.items():\n",
    "        c_tf_idf[lemma] /= c_tf_idf_sum\n",
    "\n",
    "    # Update the probabilities of the lemmas based on their c-TF-IDF scores\n",
    "    new_lemma_prob = {}\n",
    "    for lemma, prob in lemma_prob.items():\n",
    "        if lemma in c_tf_idf:\n",
    "            new_lemma_prob[lemma] = c_tf_idf[lemma]\n",
    "    \n",
    "    return new_lemma_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wordcloud\n",
    "\n",
    "def get_topic_words(bertopic_model, topic, top_n=10):\n",
    "    \"\"\"\n",
    "    Get the top n words for a given topic.\n",
    "\n",
    "    This function takes as input a BERTopic model `topic_model`, a topic number `topic`, and an optional integer parameter `top_n` specifying the number of words to return. The function returns the top n words for the given topic, along with their probabilities, as a list of tuples.\n",
    "\n",
    "    The function first retrieves the c-TF-IDF matrix and feature names from the BERTopic model. Then, it gets the row of the c-TF-IDF matrix corresponding to the given topic and uses it to find the indices of the top n words. Finally, it retrieves the words and their probabilities and returns them as a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "        topic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        topic (int): The topic number for which to retrieve the top n words.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains a word (str) and its probability (float).\n",
    "    \"\"\"\n",
    "    # Get the c-TF-IDF matrix and feature names\n",
    "    c_tf_idf = bertopic_model.c_tf_idf_.toarray()\n",
    "    feature_names = bertopic_model.vectorizer_model.get_feature_names_out()\n",
    "    \n",
    "    # Get the row of the c-TF-IDF matrix corresponding to the topic\n",
    "    topic_row = c_tf_idf[topic]\n",
    "    \n",
    "    # Get the indices of the top_n words for the topic\n",
    "    top_n_indices = np.argsort(topic_row)[::-1][:top_n]\n",
    "    \n",
    "    # Get the words and their probabilities\n",
    "    words = [feature_names[i] for i in top_n_indices]\n",
    "    probabilities = [topic_row[i] for i in top_n_indices]\n",
    "    \n",
    "    # Return the words and their probabilities as a list of tuples\n",
    "    return list(zip(words, probabilities))\n",
    "\n",
    "def group_docs_by_topic(docs, bertopic_model):\n",
    "    \"\"\"\n",
    "    Group documents by their assigned topic.\n",
    "\n",
    "    This function takes as input a list of documents `docs` and a BERTopic model `bertopic_model`. It creates a dictionary where the keys are topic numbers and the values are lists of documents assigned to each topic.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        bertopic_model (BERTopic): The BERTopic model used to assign topics to the documents.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are topic numbers (int) and the values are lists of documents (list) assigned to each topic.\n",
    "    \"\"\"\n",
    "    docs_by_topic = {}\n",
    "    for doc, topic in zip(docs, bertopic_model.topics_):\n",
    "        if topic+1 not in docs_by_topic:\n",
    "            docs_by_topic[topic+1] = []\n",
    "        docs_by_topic[topic+1].append(doc)\n",
    "    \n",
    "    return docs_by_topic \n",
    "\n",
    "def get_word_freq(bertopic_model, docs, topic, top_n=10, scale=1, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Get the word frequencies for a given topic.\n",
    "\n",
    "    This function takes as input a BERTopic model `bertopic_model`, a list of documents `docs`, a topic number `topic`, an optional integer parameter `top_n` specifying the number of words to include, an optional float parameter `scale` used to scale the probabilities of the words and an optional boolean parameter `lemmatize` which determines whether to lemmatize the words before calculating their frequencies.\n",
    "\n",
    "    The function first retrieves the top n words for the given topic using the `get_topic_words` function and scales their probabilities using the provided `scale` parameter. If `lemmatize` is `True`, the function lemmatizes the words using the `lemmatize_words` function and recalculates their probabilities using the `recalculate_probabilities` function. Otherwise, it uses the original words and their probabilities.\n",
    "\n",
    "    Parameters:\n",
    "        bertopic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        topic (int): The topic number for which to calculate the word frequencies.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to include. Defaults to 10.\n",
    "        scale (float): An optional float parameter used to scale the probabilities of the words. Defaults to 1.\n",
    "        lemmatize (bool): An optional boolean parameter used to determine whether to lemmatize the words before calculating their frequencies. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the words/lemmas (str) and the values are their probabilities (float).\n",
    "    \"\"\"\n",
    "    # Get the topic words and their probabilities\n",
    "    topic_words = get_topic_words(bertopic_model, topic, top_n=top_n)\n",
    "    # Scale the probabilities\n",
    "    topic_words = [(word, prob ** scale) for word, prob in topic_words]\n",
    "\n",
    "    if lemmatize:\n",
    "        # Group documents by their assigned topic.\n",
    "        docs_by_topic = group_docs_by_topic(docs, bertopic_model)\n",
    "        # get the documents assigned to a specific topic\n",
    "        my_docs = docs_by_topic.get(topic, [])\n",
    "        # Lemmatize the words and combine their probabilities\n",
    "        lemma_prob = lemmatize_words(topic_words)\n",
    "        # Recalculate the c-TF-IDF scores for the lemmas\n",
    "        topic_words_lemma = recalculate_probabilities(lemma_prob, my_docs, bertopic_model)\n",
    "        # Create a dictionary with the lemmas and their probabilities\n",
    "        word_freq = {lemma: prob for lemma, prob in topic_words_lemma.items()}\n",
    "    \n",
    "    else:\n",
    "        # Create a dictionary with the words and their probabilities\n",
    "        word_freq = {word: prob for word, prob in topic_words}\n",
    "    \n",
    "    return word_freq \n",
    "\n",
    "def create_wordcloud(bertopic_model, docs, topic, top_n=10, scale=1, lemmatize=False, stopwords=None, wordcloud_kwargs=None):\n",
    "    \"\"\"\n",
    "    Create a word cloud from a BERTopic model and a topic.\n",
    "\n",
    "    This function takes as input a BERTopic model `topic_model`, a list of documents `docs`, a topic number `topic`, an optional integer parameter `top_n` specifying the number of words to include in the word cloud, an optional float parameter `scale` used to scale the probabilities of the words, an optional boolean parameter `lemmatize` which determines whether to lemmatize the words before creating the word cloud, an optional list of stopwords `stopwords` to be removed from the word cloud, and an optional dictionary of keyword arguments `wordcloud_kwargs` to be passed to the WordCloud constructor.\n",
    "\n",
    "    The function first retrieves the top n words for the given topic using the `get_topic_words` function and scales their probabilities using the provided `scale` parameter. If `lemmatize` is `True`, the function lemmatizes the words using the `lemmatize_words` function and recalculates their probabilities using the `recalculate_probabilities` function. Otherwise, it uses the original words and their probabilities.\n",
    "\n",
    "    The function then removes any stopwords from the list of words (if provided) and creates a word cloud using the WordCloud class from the wordcloud library. The resulting word cloud is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        topic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        topic (int): The topic number for which to create the word cloud.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to include in the word cloud. Defaults to 10.\n",
    "        scale (float): An optional float parameter used to scale the probabilities of the words. Defaults to 1.\n",
    "        lemmatize (bool): An optional boolean parameter used to determine whether to lemmatize the words before creating the word cloud. Defaults to False.\n",
    "        stopwords (list): An optional list of stopwords to be removed from the word cloud. Defaults to None.\n",
    "        wordcloud_kwargs (dict): An optional dictionary of keyword arguments to be passed to the WordCloud constructor. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        wordcloud.WordCloud: The resulting word cloud.\n",
    "    \"\"\"\n",
    "    # Get the word frequencies for a given topic.\n",
    "    word_freq = get_word_freq(bertopic_model, docs, topic, top_n=top_n, scale=scale, lemmatize=lemmatize)\n",
    "\n",
    "    # Remove stopwords from word_freq if provided\n",
    "    if stopwords:\n",
    "        word_freq = {word: freq for word, freq in word_freq.items() if word not in stopwords}\n",
    "    \n",
    "    # Create the word cloud using the words/lemmas and their probabilities\n",
    "    wc = WordCloud(**(wordcloud_kwargs or {}))\n",
    "    wc.generate_from_frequencies(word_freq)\n",
    "\n",
    "    # Display the word cloud\n",
    "    # plt.imshow(wc, interpolation='bilinear')\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "    return wc\n",
    "\n",
    "def create_wordclouds_bertopic(bertopic_model, docs, top_n=10, scale=1, lemmatize=False, stopwords=None, wordcloud_kwargs=None, to_save=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Create word clouds for all topics in a BERTopic model.\n",
    "\n",
    "    This function takes as input a BERTopic model `bertopic_model`, a list of documents `docs`, an optional integer parameter `top_n` specifying the number of words to include in each word cloud, an optional float parameter `scale` used to scale the probabilities of the words, an optional boolean parameter `lemmatize` which determines whether to lemmatize the words before creating the word clouds, an optional list of stopwords `stopwords` to be removed from the word clouds, an optional dictionary of keyword arguments `wordcloud_kwargs` to be passed to the WordCloud constructor, an optional boolean parameter `to_save` which determines whether to save the word clouds as image files, and an optional string parameter `save_path` specifying the path where the image files should be saved.\n",
    "\n",
    "    The function first retrieves the topic information from the BERTopic model and sets the index of the resulting DataFrame to be the topic number. Then, it loops over the topic numbers in the DataFrame and calls the `create_wordcloud` function to create a word cloud for each topic. If `to_save` is `True`, it saves each word cloud as an image file at the specified location using the custom name of the topic.\n",
    "\n",
    "    The resulting dictionary, where the keys are the custom names of the topics and the values are their corresponding word clouds, is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        bertopic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to include in each word cloud. Defaults to 10.\n",
    "        scale (float): An optional float parameter used to scale the probabilities of the words. Defaults to 1.\n",
    "        lemmatize (bool): An optional boolean parameter used to determine whether to lemmatize the words before creating the word clouds. Defaults to False.\n",
    "        stopwords (list): An optional list of stopwords to be removed from each word cloud. Defaults to None.\n",
    "        wordcloud_kwargs (dict): An optional dictionary of keyword arguments to be passed to each WordCloud constructor. Defaults to None.\n",
    "        to_save (bool): An optional boolean parameter used to determine whether to save each word cloud as an image file. Defaults to False.\n",
    "        save_path (str): An optional string parameter specifying the path where each image file should be saved. Only used if `to_save` is True. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the custom names of the topics (str) and the values are their corresponding word clouds (wordcloud.WordCloud).\n",
    "    \"\"\"\n",
    "    # Check that save_path is provided if to_save is True\n",
    "    if to_save and save_path is None:\n",
    "        raise ValueError(\"If to_save is True, save_path must be provided\")\n",
    "    \n",
    "    # Get the topic information\n",
    "    topic_info = bertopic_model.get_topic_info()\n",
    "    # Set the index of the DataFrame to be the topic number\n",
    "    topic_info = topic_info.set_index('Topic')\n",
    "    wc_pics={}\n",
    "    # Loop over the topic numbers in the DataFrame\n",
    "    for topic_number in topic_info.index:\n",
    "        # Get the custom name of the current topic for saving purpose\n",
    "        topic_custom_name = topic_info.loc[topic_number, 'CustomName']\n",
    "        wc_pic = create_wordcloud(bertopic_model, docs, topic_number+1, top_n=top_n, scale=scale, lemmatize=lemmatize, stopwords=stopwords, wordcloud_kwargs=wordcloud_kwargs)\n",
    "        wc_pics.update({topic_custom_name: wc_pic})\n",
    "\n",
    "        if to_save and lemmatize:\n",
    "            wc_pic.to_file(f'{save_path}/{topic_custom_name}_lemmatized.png')\n",
    "        elif to_save and lemmatize==False:\n",
    "            wc_pic.to_file(f'{save_path}/{topic_custom_name}.png')\n",
    "\n",
    "    return wc_pics\n",
    "\n",
    "# examples of default colors for the wordcloud\n",
    "# colormaps = [\n",
    "#     'viridis', 'plasma', 'inferno', 'magma', 'cividis',\n",
    "#     'Greys', 'Purples', 'Blues', 'Greens', 'Oranges',\n",
    "#     'Reds', 'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd',\n",
    "#     'RdPu', 'BuPu', 'GnBu', 'PuBu', 'YlGnBu',\n",
    "#     'PuBuGn', 'BuGn', 'YlGn'\n",
    "# ]\n",
    "\n",
    "# Create a custom colormap\n",
    "# colors = [\"#294D61\", \"#6DA5C0\", \"#0F969C\", \"#0C7075\", \"#072E33\",\"#05161A\"]\n",
    "colors = [\"#F1916D\", \"#F5D7DB\", \"#BD83B8\", \"#473E66\", \"#1B3358\",\"#06142E\"]\n",
    "my_cmap = ListedColormap(colors)\n",
    "\n",
    "# Create a binary mask from an image\n",
    "# make sure that the image is black and white, where black pixels indicate where to draw words and white pixels indicate where not to draw words.\n",
    "# mask = np.array(Image.open('circle.png'))\n",
    "\n",
    "wordcloud_stopwords = [\"get\",\"would\",\"us\",\"good\",\"like\",\"goods\",\"got\",\"able\",\"quite\",\"always\",\"nothing\",\"add\",\"everything\",\"think\",\"gave\",\"due\",\"find\",\"say\",\"took\",\"still\",\"within\",\"22\",\"10\",\"one\",\"makhloufi\",\"saliger\",\"kessler\",\"naldrin\",\"hidde\",\"mohua\",\"possible\",\"since\",\"could\",\"especially\",\"altivar\",\"every\",\"anyway\",\"egawa\",\"sometimes\"]\n",
    "\n",
    "wordcloud_kwargs = {\n",
    "    'width': 800,\n",
    "    'height': 400,\n",
    "    'background_color': 'black',\n",
    "    'contour_width': 100,\n",
    "    'contour_color': 'red',\n",
    "    'colormap': 'GnBu',\n",
    "    \"contour_width\": 5,\n",
    "    \"contour_color\": 'red'\n",
    "    # \"mask\":mask\n",
    "}\n",
    "\n",
    "wordclouds = create_wordclouds_bertopic(topic_model_merged, docs, \n",
    "                           top_n=50, scale=1, \n",
    "                           lemmatize=True, \n",
    "                           stopwords=wordcloud_stopwords, \n",
    "                           wordcloud_kwargs=wordcloud_kwargs,\n",
    "                           to_save=True, \n",
    "                           save_path=\"../data/wordclouds/test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barchart from seaborn : emotions by year\n",
    "\n",
    "# # Create a count plot\n",
    "# sns.set(style=\"whitegrid\")\n",
    "# g = sns.catplot(x=\"year\", y=None, hue=\"single_emotion_label\", data=df4, kind=\"count\", height=4, aspect=2)\n",
    "\n",
    "# # Set the axis labels\n",
    "# g.set_axis_labels(\"Year\", \"Count\")\n",
    "\n",
    "# # Set the title\n",
    "# plt.title(\"Count of Emotions by Year\")\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph avec plotly.express : barchart for one defined emotion\n",
    "\n",
    "# import ipywidgets as widgets\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_emotion(df, emotion):\n",
    "    \"\"\"\n",
    "    Plot a histogram of the distribution of a given emotion by year.\n",
    "\n",
    "    This function takes as input a dataframe `df`, an emotion `emotion` and creates a histogram showing the distribution of this emotion by year. The data is filtered to only include rows with the specified emotion, and a histogram is created using the Plotly Express library. The axis labels and title are set, and the resulting plot is shown.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input dataframe containing the data to be plotted.\n",
    "        emotion (str): The emotion to be plotted.\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objs.Figure: The resulting histogram plot.\n",
    "    \"\"\"\n",
    "    # Filter the data to only include rows with the specified emotion\n",
    "    filtered_data = df[df['single_emotion_label'] == emotion]\n",
    "    \n",
    "    # Create a histogram\n",
    "    fig = px.histogram(filtered_data, x='year', nbins=20)\n",
    "    \n",
    "    # Set the axis labels\n",
    "    fig.update_layout(xaxis_title='Year', yaxis_title='Count')\n",
    "    \n",
    "    # Set the title\n",
    "    fig.update_layout(title=f'Distribution of {emotion} by Year')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# # Call the function with the desired emotion\n",
    "plot_emotion(df4, 'disappointment')\n",
    "\n",
    "## Tests graph précédent avec menu déroulant pour choisir son emotion\n",
    "\n",
    "# # Create a dropdown menu with all unique emotions in 'single_emotion_label' column\n",
    "# emotions = df4['single_emotion_label'].unique()\n",
    "# dropdown = widgets.Dropdown(options=emotions)\n",
    "\n",
    "# # Create an output widget to display the plot\n",
    "# output = widgets.Output()\n",
    "\n",
    "# # Define a function that updates the plot when a new emotion is selected\n",
    "# def on_change(change):\n",
    "#     if change['name'] == 'value':\n",
    "#         with output:\n",
    "#             output.clear_output()\n",
    "#             plot_emotion(change['new'])\n",
    "\n",
    "# # Register the function as a callback for changes in the dropdown value\n",
    "# dropdown.observe(on_change)\n",
    "\n",
    "# # # Display the widgets\n",
    "# display(dropdown)\n",
    "# display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph barchart of one given emotion by a specified class (year, zone, etc.)\n",
    "\n",
    "# Plot a bar chart or histogram of the distribution of a given emotion by a specified class\n",
    "def plot_emotion(df, emotion, class_name, time_period=None, random_colors=True, set_colors=['#96ceb4', '#87bdd8', '#ffcc5c', '#ff6f69', '#f4a688', '#d96459'], set_color=None):\n",
    "    \"\"\"\n",
    "    Plot a bar chart or histogram of the distribution of a given emotion by a specified class.\n",
    "\n",
    "    This function takes as input a dataframe `df`, an emotion `emotion`, a class name `class_name` representing the column by which to group the data, an optional time period `time_period` specifying the year to filter the data by, an optional boolean parameter `random_colors` which determines whether to use a random color for the plot or a specified color, an optional list of colors `set_colors` to choose from if `random_colors` is `True`, and an optional color `set_color` to use if `random_colors` is `False`.\n",
    "\n",
    "    The data is filtered to only include rows with the specified emotion, and further filtered by the specified time period if provided. If `random_colors` is `True`, a random color is chosen from the provided list of colors. Otherwise, the specified color is used.\n",
    "\n",
    "    If `class_name` is not 'year', the function creates a bar chart showing the distribution of the emotion by the specified class. Otherwise, it creates a histogram showing the distribution of the emotion by year. The axis labels and title are set, and the resulting plot is returned.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input dataframe containing the data to be plotted.\n",
    "        emotion (str): The emotion to be plotted.\n",
    "        class_name (str): The name of the column in `df` representing the class by which to group the data.\n",
    "        time_period (int): An optional integer parameter specifying the year to filter the data by. Only used if `class_name` is not 'year'. Defaults to None.\n",
    "        random_colors (bool): An optional boolean parameter used to determine whether to use a random color for the plot or a specified color. Defaults to True.\n",
    "        set_colors (list): An optional list of colors to choose from if `random_colors` is True. Defaults to ['#96ceb4', '#87bdd8', '#ffcc5c', '#ff6f69', '#f4a688', '#d96459'].\n",
    "        set_color (str): An optional color to use if `random_colors` is False. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objs.Figure: The resulting bar chart or histogram plot.\n",
    "    \"\"\"\n",
    "    # Check that set_color is provided if random_colors is False\n",
    "    if random_colors==False and set_color is None:\n",
    "        raise ValueError(\"set_color must be provided if random_colors is False\")\n",
    "    \n",
    "    # Filter the data to only include rows with the specified emotion\n",
    "    filtered_data = df[df['single_emotion_label'] == emotion]\n",
    "\n",
    "    # Filter the data by the specified time period if provided\n",
    "    if time_period != None:\n",
    "        if class_name == 'year':\n",
    "            raise ValueError(\"class_name cannot be 'year' when time_period is defined\")\n",
    "        filtered_data = filtered_data[filtered_data['year'] == time_period]\n",
    "\n",
    "    if random_colors:\n",
    "        # Randomly choose a color from the defined colors list\n",
    "        colors_list = set_colors\n",
    "        color = random.choice(colors_list)\n",
    "    else:\n",
    "        color = set_color\n",
    "\n",
    "    # Sort the data by ascending frequency if class_name is not 'year'\n",
    "    if class_name != 'year':\n",
    "        filtered_data = filtered_data[class_name].value_counts(ascending=True).reset_index()\n",
    "        filtered_data.columns = [class_name, 'count']\n",
    "        fig = px.bar(filtered_data, x=class_name, y='count', color_discrete_sequence=[color], width=1100, height=600)\n",
    "    else:\n",
    "        # Create a histogram\n",
    "        fig = px.histogram(filtered_data, x=class_name, nbins=20, color_discrete_sequence=[color], width=1100, height=600)\n",
    "\n",
    "    # Set the axis labels\n",
    "    fig.update_layout(xaxis_title=class_name, yaxis_title='Count')\n",
    "    # Set the title\n",
    "    fig.update_layout(title=f'Distribution of {emotion} by {class_name}' + (f' in {time_period}' if time_period is not None else ''))\n",
    "    # Center the title\n",
    "    fig.update_layout(title_x=0.5)\n",
    "\n",
    "    return fig\n",
    "\n",
    "emotions = df4['single_emotion_label'].unique()\n",
    "class_name = \"Account Country\"\n",
    "time_period = 2023\n",
    "# Loop over each emotion and save the corresponding graph\n",
    "for emotion in emotions:\n",
    "    fig = plot_emotion(df4, emotion, class_name, time_period)\n",
    "    # Export the chart as a PNG image\n",
    "    pio.write_image(fig, f'../data/graphs/test/emotions/{emotion}_by_{class_name}_{time_period}.png')\n",
    "\n",
    "# plot_emotion(df4, \"love\", \"Account Country\", time_period=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['systematic ar + deadlines respected',\n",
       " 'deadlines not respected and uncertain, no reliable information',\n",
       " 'the order is not completely delivered',\n",
       " 'correct and on delivery time',\n",
       " 'sending a \"deadline\" satisfaction questionnaire when you are late for several months is a matter of psychiatry. opening case is useless. you make delivery commitments that you don\\'t keep. you close the cases without notifying me. the manual allocations are allocated to the largest.',\n",
       " 'the impossibility of having deadlines on the articles in allocation. deadline respected',\n",
       " \"catastrophic management. i'm still not delivered\",\n",
       " 'on 25/10/21 mr verollet explains that a 1st part is carried out the second part will take place at the end of october at the beginning of november prefers reminder mid-november in order to take stock of the service in its entirety./ on 14/12/21 mr verollet gives 9/10 for the 2 parties for the availability of the teams and the skills of the people who intervened, totally adapted to the problem. they were able to solve the various problems in a fairly short time, the realization is therefore satisfactory the only point to improve is to lengthen the time to report problems after mes. the mes is carried out in a short time and the validation should be longer. we have a contract but problems related to mes appear 2 months later which requires going through another specific modification maintenance contract. this is the only negative point of the problems related to the mes should be linked to the initial contract.',\n",
       " 'delays in delivery',\n",
       " 'fast delivery, package arrives in good condition, well packaged material. deadlines respected']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_top_topic_docs(probabilities, df, topic, top_n_docs=len(df)):\n",
    "    \"\"\"\n",
    "    Get the top n documents for a specified topic.\n",
    "\n",
    "    Parameters:\n",
    "    probabilities (numpy.ndarray): The probabilities from the BERTopic model.\n",
    "    df (pandas.DataFrame): The dataframe containing the documents.\n",
    "    topic (int): The topic to get the top n documents for.\n",
    "    top_n_docs (int): The number of top documents to return. Defaults to the length of df.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing a dataframe with the top n documents for the specified topic and a list with their content.\n",
    "    \"\"\"\n",
    "    def sort_docs_with_same_prob(probabilities, prob, topic):\n",
    "        \"\"\"\n",
    "        Sort documents that have the same probability for a specified topic.\n",
    "\n",
    "        Parameters:\n",
    "        probabilities (numpy.ndarray): The probabilities from the BERTopic model.\n",
    "        prob (float): The probability to filter by.\n",
    "        topic (int): The topic to sort the documents for.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: An array containing the sorted indices of the documents that have the specified probability for the specified topic.\n",
    "        \"\"\"\n",
    "        # Get the indices of all documents that have the current probability for the defined topic\n",
    "        top_docs_indices = np.where(probabilities[:, topic] == prob)[0]\n",
    "        # Sort the probabilities for each document in descending order\n",
    "        sorted_probs = -np.sort(-probabilities[top_docs_indices], axis=1)\n",
    "        # Compute the score for each document based on the difference between their first and second scores\n",
    "        scores = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
    "        # Sort these top documents based on their scores\n",
    "        sorted_top_docs_indices = top_docs_indices[np.argsort(scores)[::-1]]\n",
    "        \n",
    "        return sorted_top_docs_indices\n",
    "    \n",
    "    # Get the unique probabilities for the defined topic\n",
    "    unique_probs = np.unique(probabilities[:, topic])\n",
    "    # Sort the unique probabilities in descending order\n",
    "    sorted_unique_probs = np.sort(unique_probs)[::-1]\n",
    "    \n",
    "    # Initialize an empty list to store the sorted indices of all documents\n",
    "    sorted_docs_indices = []\n",
    "    \n",
    "    # Loop over the unique probabilities\n",
    "    for prob in sorted_unique_probs:\n",
    "        # Sort the documents that have the current probability for the defined topic\n",
    "        sorted_top_docs_indices = sort_docs_with_same_prob(probabilities, prob, topic)\n",
    "        # Append these indices to the list of sorted indices of all documents\n",
    "        sorted_docs_indices.extend(sorted_top_docs_indices)\n",
    "    \n",
    "    # Take the top n from this sorted list\n",
    "    final_top_docs_indices = np.array(sorted_docs_indices)[:top_n_docs]\n",
    "    \n",
    "    # Get the content of the top n documents from your dataframe\n",
    "    top_docs_df = df.iloc[final_top_docs_indices]\n",
    "    top_docs_content = top_docs_df[\"processed_data\"].to_list()\n",
    "    \n",
    "    return top_docs_df, top_docs_content\n",
    "\n",
    "def filter_docs(df, filter_column, filter_value):\n",
    "    \"\"\"\n",
    "    Filter a dataframe based on a specified column and value.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to filter.\n",
    "    filter_column (str): The name of the column to filter by.\n",
    "    filter_value (str): The value to filter by in the specified column.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the filtered dataframe and a boolean mask indicating which rows match the specified filter.\n",
    "    \"\"\"\n",
    "    filter_mask = df[filter_column] == filter_value\n",
    "    return df[filter_mask], filter_mask\n",
    "\n",
    "# Filter the top docs based on the specified column and value\n",
    "filtered_top_docs, filter_mask = filter_docs(df4, \"Account Country\", \"France\")\n",
    "top_docs_df, top_docs_content = get_top_topic_docs(topic_model_merged.probabilities_[filter_mask], filtered_top_docs, topic=2, top_n_docs=10)\n",
    "\n",
    "top_docs_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wassati",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
