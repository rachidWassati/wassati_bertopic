{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: wordcloud\n",
      "Version: 1.9.2\n",
      "Summary: A little word cloud generator\n",
      "Home-page: https://github.com/amueller/word_cloud\n",
      "Author: Andreas Mueller\n",
      "Author-email: t3kcit+wordcloud@gmail.com\n",
      "License: MIT\n",
      "Location: /home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages\n",
      "Requires: matplotlib, numpy, pillow\n",
      "Required-by: stylecloud\n"
     ]
    }
   ],
   "source": [
    "!pip show wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UMAP' from 'umap' (/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbertopic\u001b[39;00m \u001b[39mimport\u001b[39;00m BERTopic\n\u001b[1;32m     13\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available()\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbertopic\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_bertopic\u001b[39;00m \u001b[39mimport\u001b[39;00m BERTopic\n\u001b[1;32m      3\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.15.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBERTopic\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/_bertopic.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m# Models\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhdbscan\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mumap\u001b[39;00m \u001b[39mimport\u001b[39;00m UMAP\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m sklearn_version\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UMAP' from 'umap' (/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle, ast\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline, DataCollatorWithPadding\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the excel review file\n",
    "df = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_labelled.csv\")\n",
    "df2 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_sentiment_labelled.csv\")\n",
    "df3 = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_emotions_labelled.csv\")\n",
    "\n",
    "# Put the data in the correct format for bertopic\n",
    "docs = df3[\"allComment\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, max_length=512, truncation=True)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for review in [[x] for x in df.allComment.tolist()]:\n",
    "    prediction = classifier(review)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "res = pd.DataFrame([item for sublist in predictions for item in sublist])\n",
    "df_pred = pd.concat([df, res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_pred.copy()\n",
    "# Rename the second 'label' column to 'sentiment_label'\n",
    "cols = df2.columns.tolist()\n",
    "cols[len(cols) - 1 - cols[::-1].index('label')] = 'sentiment_label'\n",
    "df2.columns = cols\n",
    "df2.columns\n",
    "# df2.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_sentiment_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a CUDA-enabled GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\", max_length=512)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a list of label names\n",
    "label_emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise','neutral']\n",
    "\n",
    "# Initialize lists to store the predicted labels and scores\n",
    "predicted_labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "df3 = df2\n",
    "# Iterate over the rows of the DataFrame in batches\n",
    "for i in range(0, len(df3), batch_size):\n",
    "    batch = df3[i:i+batch_size]\n",
    "    texts = batch['allComment'].tolist()\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move the inputs to the GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits.sigmoid().detach().cpu().numpy()\n",
    "    \n",
    "    # Apply a threshold to the probabilities to get the predicted labels\n",
    "    threshold = 0.5\n",
    "    labels = [[label_emotions[i] for i, prob in enumerate(prob_row) if prob > threshold] for prob_row in probs]\n",
    "    \n",
    "    # Store the predicted labels and scores\n",
    "    predicted_labels.extend(labels)\n",
    "    scores = [{label_emotions[i]: prob for i, prob in enumerate(prob_row)} for prob_row in probs]\n",
    "    # predicted_scores.extend(probs.tolist())\n",
    "    predicted_scores.extend(scores)\n",
    "\n",
    "# Add the predicted labels and scores as new columns in the DataFrame\n",
    "df3['predicted_labels'] = predicted_labels\n",
    "df3['predicted_scores'] = predicted_scores\n",
    "\n",
    "# Convert the string values in the 'predicted_labels' column into lists\n",
    "df3['predicted_labels'] = df3['predicted_labels'].apply(lambda x: ast.literal_eval(x))\n",
    "# Convert the string values in the 'predicted_scores' column into dictionaries\n",
    "df3['predicted_scores'] = df3['predicted_scores'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# df3.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_emotions_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.single_emotion_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emotions = ['admiration','approval','gratitude','caring','realization','joy','optimism','love','excitement','amusement','relief']\n",
    "negative_emotions = ['disappointment','disapproval','annoyance','confusion','nervousness','fear','sadness','remorse','disgust','embarrassment','anger']\n",
    "neutral_emotions = ['neutral','desire','surprise','curiosity']\n",
    "\n",
    "# Convert the string values in the 'predicted_labels' column into lists\n",
    "df3['predicted_labels'] = df3['predicted_labels'].apply(lambda x: ast.literal_eval(x))\n",
    "# Convert the string values in the 'predicted_scores' column into dictionaries\n",
    "df3['predicted_scores'] = df3['predicted_scores'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Create a new column 'single_emotion_label' that contains the label with the highest score\n",
    "df3['single_emotion_label'] = df3['predicted_scores'].apply(lambda x: max(x, key=x.get))\n",
    "# Create a new column 'sentiment' that contains the sentiment of the emotion in the 'single_emotion_label' column\n",
    "df3['single_sentiment_label'] = df3['single_emotion_label'].apply(lambda x: 'positive' if x in positive_emotions else ('negative' if x in negative_emotions else ('neutral' if x in neutral_emotions else 'unknown')))\n",
    "\n",
    "# Use the explode() method to transform each element of the list into a row\n",
    "new_df3 = df3.explode('predicted_labels')\n",
    "new_df3 = new_df3.rename(columns={'predicted_labels': 'emotion_label'})\n",
    "\n",
    "# Pour retourner au df initial avec une liste de label\n",
    "# Group the rows by the original index and aggregate the 'emotion_label' column into a list\n",
    "# original_df = new_df.groupby(new_df.index).agg({'emotion_label': list})\n",
    "new_df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of rows for each sentiment in the 'sentiment_label' column\n",
    "total_positive = len(df3[df3['sentiment_label'] == 'positive'])\n",
    "total_negative = len(df3[df3['sentiment_label'] == 'negative'])\n",
    "total_neutral = len(df3[df3['sentiment_label'] == 'neutral'])\n",
    "\n",
    "# Calculate the number of matches for each sentiment\n",
    "positive_matches = len(df3[(df3['sentiment_label'] == 'positive') & (df3['single_sentiment_label'] == 'positive')])\n",
    "negative_matches = len(df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_label'] == 'negative')])\n",
    "neutral_matches = len(df3[(df3['sentiment_label'] == 'neutral') & (df3['single_sentiment_label'] == 'neutral')])\n",
    "\n",
    "# Calculate the percentage of matches for each sentiment\n",
    "positive_match_percent = positive_matches / total_positive * 100 if total_positive > 0 else 0\n",
    "negative_match_percent = negative_matches / total_negative * 100 if total_negative > 0 else 0\n",
    "neutral_match_percent = neutral_matches / total_neutral * 100 if total_neutral > 0 else 0\n",
    "\n",
    "# Calculate the number of differences for each sentiment\n",
    "positive_differences = total_positive - positive_matches\n",
    "negative_differences = total_negative - negative_matches\n",
    "neutral_differences = total_neutral - neutral_matches\n",
    "\n",
    "# Calculate the percentage of differences for each sentiment\n",
    "positive_difference_percent = positive_differences / total_positive * 100 if total_positive > 0 else 0\n",
    "negative_difference_percent = negative_differences / total_negative * 100 if total_negative > 0 else 0\n",
    "neutral_difference_percent = neutral_differences / total_neutral * 100 if total_neutral > 0 else 0\n",
    "\n",
    "# Calculate the distribution of differences for each sentiment\n",
    "positive_to_negative = len(df3[(df3['sentiment_label'] == 'positive') & (df3['single_sentiment_label'] == 'negative')])\n",
    "positive_to_neutral = len(df3[(df3['sentiment_label'] == 'positive') & (df3['single_sentiment_label'] == 'neutral')])\n",
    "negative_to_positive = len(df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_label'] == 'positive')])\n",
    "negative_to_neutral = len(df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_label'] == 'neutral')])\n",
    "neutral_to_positive = len(df3[(df3['sentiment_label'] == 'neutral') & (df3['single_sentiment_label'] == 'positive')])\n",
    "neutral_to_negative = len(df3[(df3['sentiment_label'] == 'neutral') & (df3['single_sentiment_label'] == 'negative')])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Positive matches: {positive_match_percent:.2f}%\")\n",
    "print(f\"Negative matches: {negative_match_percent:.2f}%\")\n",
    "print(f\"Neutral matches: {neutral_match_percent:.2f}%\")\n",
    "print(f\"Positive differences: {positive_difference_percent:.2f}%\")\n",
    "print(f\"Negative differences: {negative_difference_percent:.2f}%\")\n",
    "print(f\"Neutral differences: {neutral_difference_percent:.2f}%\")\n",
    "print(f\"Positive to Negative: {positive_to_negative / total_positive * 100:.2f}%\" if total_positive > 0 else \"Positive to Negative: N/A\")\n",
    "print(f\"Positive to Neutral: {positive_to_neutral / total_positive * 100:.2f}%\" if total_positive > 0 else \"Positive to Neutral: N/A\")\n",
    "print(f\"Negative to Positive: {negative_to_positive / total_negative * 100:.2f}%\" if total_negative > 0 else \"Negative to Positive: N/A\")\n",
    "print(f\"Negative to Neutral: {negative_to_neutral / total_negative * 100:.2f}%\" if total_negative > 0 else \"Negative to Neutral: N/A\")\n",
    "print(f\"Neutral to Positive: {neutral_to_positive / total_neutral * 100:.2f}%\" if total_neutral > 0 else \"Neutral to Positive: N/A\")\n",
    "print(f\"Neutral to Negative: {neutral_to_negative / total_neutral * 100:.2f}%\" if total_neutral > 0 else \"Neutral to Negative: N/A\")\n",
    "\n",
    "# Filter the DataFrame to only include rows where the 'sentiment_label' is 'negative' and the 'single_sentiment_label' is 'neutral'\n",
    "negative_to_neutral = df3[(df3['sentiment_label'] == 'negative') & (df3['single_sentiment_label'] == 'neutral')]\n",
    "\n",
    "# Calculate the distribution of emotions in the 'single_emotion_label' column\n",
    "emotion_distribution = negative_to_neutral['single_emotion_label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "emotion_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bertopic_model(filename):\n",
    "    \"\"\"\n",
    "    Load a BERTopic model and associated data from a file.\n",
    "    \n",
    "    :param filename: The name of the file to load the data from.\n",
    "    :return: A tuple containing the loaded BERTopic model, topics, probs, and docs variables.\n",
    "    \"\"\"\n",
    "    # Load the BERTopic model\n",
    "    topic_model = BERTopic.load(filename)\n",
    "    \n",
    "    # Load the topics, probs, and docs variables\n",
    "    with open(filename + '_data.pkl', 'rb') as f:\n",
    "        topics, probs, embeddings, docs = pickle.load(f)\n",
    "    \n",
    "    return topic_model, topics, probs, embeddings, docs\n",
    "\n",
    "topic_model, topics, probs, embeddings, docs = load_bertopic_model('../models/raw_keybert_bertopic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "topics_to_merge = [ [42,3,0,13], #Delivery Deadlines : challenges and strategies involved in managing delivery deadlines in logistics operations. (vert)\n",
    "                    [20,50,27], #Quotation and Pricing Strategies (vert bas)\n",
    "                    [35,32], #Touch Panels and Screens (rouge, haut)\n",
    "                    [40,36], #Frequency Converters : frequency converters used in industrial applications and the technical support provided by manufacturers and suppliers (rouge, suite)\n",
    "                    [37,21,6,12,9,4,1,14,16,31,19], #“Automation Components” : hardware and software components used in industrial automation systems. (rouge centre)\n",
    "                    [33,46,8], #Product Evaluation : evaluate the quality, affordability and reliability of products and services (rouge, fin)\n",
    "                    [44,51,23,41,49,57,22], #Customer Support : Reliability and Quality in Customer Service and Support (bleu ciel)\n",
    "                    [58,59], #Quick Customer Service (marron)\n",
    "                    [38,10,26,52,39,43], #Problem Solving and Communication (focus on the importance of being efficient and precise when solving problems) (jaune)\n",
    "                    [45,47,55,53,54], #Assistance and Guidance (noir)\n",
    "                    [29,30,11,24], #Power Supply Issues (2e vert, haut)\n",
    "                    [7,5,2,25,15,34,18,28,17], #Technical Support (2e vert, bas)\n",
    "                    [48,56] #None : positive feedback (2e rouge)\n",
    "]\n",
    "\n",
    "# names = [\"Delivery Deadlines\",\n",
    "#     \"Pricing\", #Quotation and Pricing Strategies\n",
    "#     \"Touch Screens\", #Touch Panels and Screens\n",
    "#     \"Frequency Converters\",\n",
    "#     \"Automation Components\",\n",
    "#     \"Product Evaluation\",\n",
    "#     \"Customer Support\", #Reliability and Quality in Customer Service and Support\n",
    "#     \"Quick Customer Service\",\n",
    "#     \"Problem Solving & Comm\",\n",
    "#     \"Assistance\", #Assistance and Guidance\n",
    "#     \"Power Supply Issues\",\n",
    "#     \"Technical Support\",\n",
    "#     \"positive feedback\"]\n",
    "\n",
    "# # Create a dictionary where the keys are the topics and the values are the custom labels\n",
    "# topic_labels_dict = {}\n",
    "# topic_labels_dict[-1]=\"Outliers\"\n",
    "# for i in range(len(topics_to_merge)):\n",
    "#     for topic in topics_to_merge[i]:\n",
    "#         topic_labels_dict[topic] = names[i]\n",
    "\n",
    "topic_model_merged = copy.deepcopy(topic_model)\n",
    "topic_model_merged.merge_topics(docs, topics_to_merge)\n",
    "# topic_model_merged.set_topic_labels(topic_labels_dict)\n",
    "\n",
    "# topic_model_merged.visualize_barchart(top_n_topics=50, custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"Outliers\",\n",
    "    \"Automation Components\",\n",
    "    \"Technical Support\",\n",
    "    \"Delivery Deadlines\",\n",
    "    \"Problem Solving & Comm\",\n",
    "    \"Power Supply Issues\",\n",
    "    \"Customer Support\", #Reliability and Quality in Customer Service and Support\n",
    "    \"Product Evaluation\",\n",
    "    \"Pricing\", #Quotation and Pricing Strategies\n",
    "    \"Assistance\", #Assistance and Guidance\n",
    "    \"Touch Screens\", #Touch Panels and Screens\n",
    "    \"Frequency Converters\",\n",
    "    \"Positive feedback\",\n",
    "    \"Quick Customer Service\"\n",
    "    ]\n",
    "mergedtopic_labels_dict = {i-1: item for i, item in enumerate(label_names)}\n",
    "\n",
    "topic_model_merged.set_topic_labels(mergedtopic_labels_dict)\n",
    "\n",
    "topic_model_merged.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the topic representation of major topics per class:\n",
    "topics_per_sentiment = topic_model_merged.topics_per_class(df3[\"allComment\"].astype(str).tolist(), classes=df3.sentiment_label.to_list())\n",
    "topic_model_merged.visualize_topics_per_class(topics_per_sentiment, custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the topic representation of major topics per class:\n",
    "topics_per_emotion = topic_model_merged.topics_per_class(df3[\"allComment\"].astype(str).tolist(), classes=df3.single_emotion_label.to_list())\n",
    "topic_model_merged.visualize_topics_per_class(topics_per_emotion, top_n_topics=30, custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the topic representation of major topics per class:\n",
    "topics_per_emotion = topic_model_merged.topics_per_class(df3[\"allComment\"].astype(str).tolist(), classes=df3.market_segment.to_list())\n",
    "topic_model_merged.visualize_topics_per_class(topics_per_emotion, top_n_topics=30, custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the visualization with the original embeddings\n",
    "topic_model_merged.visualize_documents(docs, embeddings=embeddings, custom_labels=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Reduce dimensionality of embeddings, this step is optional\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model_merged.visualize_documents(docs, reduced_embeddings=reduced_embeddings, custom_labels=True, hide_annotations=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la colonne de date en un objet datetime\n",
    "df3['Response Date'] = pd.to_datetime(df3['Response Date'])\n",
    "# Extraire l'année et le mois et les stocker dans une nouvelle colonne\n",
    "df3['year_month'] = df3['Response Date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "topics_over_time = topic_model_merged.topics_over_time(docs, df3['year_month'])#, nr_bins=20)\n",
    "topic_model_merged.visualize_topics_over_time(topics_over_time, custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_merged.visualize_heatmap(custom_labels=True, n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_merged.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wassati",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
